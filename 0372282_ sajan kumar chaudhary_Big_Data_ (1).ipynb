{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErU0DHCfad3b"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import (\n",
        "    col, when, isnan, isnull, mean, stddev, percentile_approx,\n",
        "    hour, dayofweek, month, year, date_format,\n",
        "    regexp_replace, split, trim, abs as spark_abs,\n",
        "    udf, lit, coalesce, round as spark_round\n",
        ")\n",
        "from pyspark.sql.types import IntegerType, DoubleType, StringType\n",
        "from pyspark.ml.feature import Imputer, StandardScaler, VectorAssembler\n",
        "from pyspark.ml.stat import Correlation\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SmartCityTransportPreprocessing\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"=== SMART CITY TRANSPORT DATA PREPROCESSING ===\")\n",
        "print(f\"Spark Version: {spark.version}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffifLyqzbc8A",
        "outputId": "2e1600b6-f455-4475-d21f-18701f0b2ddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== SMART CITY TRANSPORT DATA PREPROCESSING ===\n",
            "Spark Version: 3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the transportation dataset\n",
        "print(\"\\n1. LOADING DATASET...\")\n",
        "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\") \\\n",
        "    .csv(\"/content/drive/MyDrive/Big_Data_public_transport_dataset_v2.csv\")\n",
        "\n",
        "print(f\"Original dataset shape: {df.count()} rows, {len(df.columns)} columns\")\n",
        "print(\"Column names:\", df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67wPyuQMbtja",
        "outputId": "6790d2a3-cc54-4698-acae-7564c8dbe8c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. LOADING DATASET...\n",
            "Original dataset shape: 30000 rows, 10 columns\n",
            "Column names: ['Route ID', 'Stop ID', 'Timestamp', 'Passenger Count', 'Latitude', 'Longitude', 'Delay Logs', 'Fare Data', 'delay_minutes', 'fare_amount']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display basic statistics\n",
        "print(\"\\n2. INITIAL DATA OVERVIEW...\")\n",
        "df.describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZXZZ2oNcAik",
        "outputId": "b9fe2c55-85e6-4b2b-bd70-84dfdfa816e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2. INITIAL DATA OVERVIEW...\n",
            "+-------+--------+-------+-----------------+------------------+-------------------+------------------+-----------------+-----------------+------------------+\n",
            "|summary|Route ID|Stop ID|  Passenger Count|          Latitude|          Longitude|        Delay Logs|        Fare Data|    delay_minutes|       fare_amount|\n",
            "+-------+--------+-------+-----------------+------------------+-------------------+------------------+-----------------+-----------------+------------------+\n",
            "|  count|   30000|  30000|            28516|             28500|              28500|             28514|            28516|            30000|             30000|\n",
            "|   mean|    NULL|   NULL|20.39931968017955|28.001348159719303|  85.40057991242078| 10.53008171424572|54.43123790152904|7.478633333333334|18.739316666666667|\n",
            "| stddev|    NULL|   NULL|100.9575101980892|0.2883255381825588|0.11553323310202271|102.05187071718426|26.97355662174156|4.612546583514189|2.3062732917570963|\n",
            "|    min|    R001|   S001|              0.0|         27.498455|          85.196462|            -18.72|            -20.0|                0|              15.0|\n",
            "|    max|    R020|   S100|            999.0|         28.501939|          85.601999|             999.0|            100.0|               15|              22.5|\n",
            "+-------+--------+-------+-----------------+------------------+-------------------+------------------+-----------------+-----------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession with HDFS\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SmartCity_Transport_Analysis\") \\\n",
        "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\") \\\n",
        "    .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "ekl_aZlEGM_B"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, isnan\n",
        "from pyspark.sql.types import StringType, DoubleType, FloatType\n",
        "\n",
        "print(\"\\n3. MISSING VALUES ANALYSIS...\")\n",
        "missing_counts = []\n",
        "\n",
        "for column in df.columns:\n",
        "    data_type = [field.dataType for field in df.schema.fields if field.name == column][0]\n",
        "\n",
        "    # Start with NULL check\n",
        "    condition = col(column).isNull()\n",
        "\n",
        "    # Add isnan() check for numeric types only\n",
        "    if isinstance(data_type, (DoubleType, FloatType)):\n",
        "        condition = condition | isnan(col(column))\n",
        "\n",
        "    # Add empty string check for StringType only\n",
        "    if isinstance(data_type, StringType):\n",
        "        condition = condition | (col(column) == \"\")\n",
        "\n",
        "    # Count and percentage\n",
        "    missing_count = df.filter(condition).count()\n",
        "    missing_percentage = (missing_count / df.count()) * 100\n",
        "    missing_counts.append((column, missing_count, missing_percentage))\n",
        "\n",
        "    print(f\"{column}: {missing_count} missing ({missing_percentage:.2f}%)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSlJU5d0cvhQ",
        "outputId": "5ab00844-3565-4fd2-be54-02a95583b0e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "3. MISSING VALUES ANALYSIS...\n",
            "Route ID: 0 missing (0.00%)\n",
            "Stop ID: 0 missing (0.00%)\n",
            "Timestamp: 0 missing (0.00%)\n",
            "Passenger Count: 1484 missing (4.95%)\n",
            "Latitude: 1500 missing (5.00%)\n",
            "Longitude: 1500 missing (5.00%)\n",
            "Delay Logs: 1486 missing (4.95%)\n",
            "Fare Data: 1484 missing (4.95%)\n",
            "delay_minutes: 0 missing (0.00%)\n",
            "fare_amount: 0 missing (0.00%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# PREPROCESSING TECHNIQUE 1: MISSING VALUE IMPUTATION\n",
        "# ===================================================="
      ],
      "metadata": {
        "id": "Zj_MTan_c84p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, last, percentile_approx\n",
        "from pyspark.sql.window import Window"
      ],
      "metadata": {
        "id": "pqqF1KmBZDwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== PREPROCESSING TECHNIQUE 1: MISSING VALUE IMPUTATION ===\")\n",
        "\n",
        "print(\"\\nJustification:\")\n",
        "print(\"- Missing values in critical fields (Passenger Count, GPS coordinates, Fare Data)\")\n",
        "print(\"- Simple deletion would lose 5% of data, reducing analysis power\")\n",
        "print(\"- Imputation preserves dataset integrity while maintaining statistical properties\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa2w7Cphc4_5",
        "outputId": "71204bb6-f7f6-4c42-8961-56fec3881438"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== PREPROCESSING TECHNIQUE 1: MISSING VALUE IMPUTATION ===\n",
            "\n",
            "Justification:\n",
            "- Missing values in critical fields (Passenger Count, GPS coordinates, Fare Data)\n",
            "- Simple deletion would lose 5% of data, reducing analysis power\n",
            "- Imputation preserves dataset integrity while maintaining statistical properties\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Strategy 1: Median imputation for numerical fields\n",
        "print(\"\\nImplementing median imputation for numerical fields...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fn8VrwD7dNY8",
        "outputId": "6424051e-5002-4572-ac1f-e6385e0104af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Implementing median imputation for numerical fields...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate medians for imputation\n",
        "passenger_median = df.select(percentile_approx(\"Passenger Count\", 0.5).alias(\"median\")).collect()[0][\"median\"]\n",
        "fare_median = df.select(percentile_approx(\"Fare Data\", 0.5).alias(\"median\")).collect()[0][\"median\"]\n",
        "delay_median = df.select(percentile_approx(\"Delay Logs\", 0.5).alias(\"median\")).collect()[0][\"median\"]\n",
        "\n",
        "print(f\"Calculated medians - Passengers: {passenger_median}, Fare: {fare_median:.2f}, Delay: {delay_median:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SevxBXFdQis",
        "outputId": "fb2725f8-0079-4418-e743-eaa2c1823ecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculated medians - Passengers: 10.0, Fare: 54.78, Delay: 0.06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply imputation\n",
        "df_imputed = df.fillna({\n",
        "    \"Passenger Count\": passenger_median,\n",
        "    \"Fare Data\": fare_median,\n",
        "    \"Delay Logs\": delay_median\n",
        "})"
      ],
      "metadata": {
        "id": "sZrraYjbdYMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Strategy  Forward-fill for GPS coordinates using route-based grouping\n",
        "print(\"\\nImplementing route-based GPS coordinate imputation...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FujbOWlUdbrN",
        "outputId": "5b230b3b-6ea6-4bfd-da57-67305b0fbafc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Implementing route-based GPS coordinate imputation...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create window specification for GPS imputation\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "window_spec = Window.partitionBy(\"Route ID\").orderBy(\"Timestamp\")"
      ],
      "metadata": {
        "id": "htY168okTw82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\") \\\n",
        "    .csv(\"/content/drive/MyDrive/Big_Data_public_transport_dataset_v2.csv\")"
      ],
      "metadata": {
        "id": "bfIj8yT-apIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_imputed = df"
      ],
      "metadata": {
        "id": "xpf2bgAWrMm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'df' is your original DataFrame\n",
        "df_imputed = df\n",
        "\n",
        "# Define your window specification\n",
        "window_spec = Window.partitionBy(\"Route ID\").orderBy(\"Timestamp\")\n",
        "\n",
        "# Now perform withColumn operations on df_imputed\n",
        "df_imputed = df_imputed.withColumn(\n",
        "    \"Latitude\",\n",
        "    when(col(\"Latitude\").isNull(), last(col(\"Latitude\"), ignorenulls=True).over(window_spec))\n",
        ")"
      ],
      "metadata": {
        "id": "VLGWb_DfrOQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify imputation results\n",
        "print(\"\\nPost-imputation missing value check:\")\n",
        "for column in df_imputed.columns:\n",
        "    missing_count = df_imputed.filter(col(column).isNull()).count()\n",
        "    print(f\"{column}: {missing_count} missing values remaining\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IlckKUOrSrk",
        "outputId": "1b45c6d4-5f29-4947-9e22-13abb2a3d4d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Post-imputation missing value check:\n",
            "Route ID: 0 missing values remaining\n",
            "Stop ID: 0 missing values remaining\n",
            "Timestamp: 0 missing values remaining\n",
            "Passenger Count: 1484 missing values remaining\n",
            "Latitude: 28501 missing values remaining\n",
            "Longitude: 1500 missing values remaining\n",
            "Delay Logs: 1486 missing values remaining\n",
            "Fare Data: 1484 missing values remaining\n",
            "delay_minutes: 0 missing values remaining\n",
            "fare_amount: 0 missing values remaining\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PREPROCESSING TECHNIQUE 2: DATA CONSISTENCY RESOLUTION\n"
      ],
      "metadata": {
        "id": "AHp9wR23rox3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== PREPROCESSING TECHNIQUE 2: DATA CONSISTENCY RESOLUTION ===\")\n",
        "\n",
        "print(\"\\nJustification:\")\n",
        "print(\"- Identified 19,086 records with inconsistent delay measurements\")\n",
        "print(\"- Two delay fields (Delay Logs vs delay_minutes) showing major discrepancies\")\n",
        "print(\"- Consistent delay measurement critical for congestion analysis and predictions\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSv4m9PircEu",
        "outputId": "072731e0-b1c7-472b-c5f9-792595068e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== PREPROCESSING TECHNIQUE 2: DATA CONSISTENCY RESOLUTION ===\n",
            "\n",
            "Justification:\n",
            "- Identified 19,086 records with inconsistent delay measurements\n",
            "- Two delay fields (Delay Logs vs delay_minutes) showing major discrepancies\n",
            "- Consistent delay measurement critical for congestion analysis and predictions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "print(\"\\nAnalyzing delay field relationships...\")\n",
        "delay_comparison = df_imputed.select(\n",
        "    \"Delay Logs\",\n",
        "    \"delay_minutes\",\n",
        "    (F.abs(col(\"Delay Logs\") - col(\"delay_minutes\"))).alias(\"delay_difference\")\n",
        ").filter(col(\"delay_difference\") > 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izD4-yuOr62C",
        "outputId": "77a44bae-fb7e-4b87-9937-a889436b41bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analyzing delay field relationships...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Records with delay difference > 5 minutes: {delay_comparison.count()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTbRXHaxr97J",
        "outputId": "b044cfe4-f2ab-428b-8563-2c312030b8e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Records with delay difference > 5 minutes: 19086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Strategy: Create unified delay field using business logic\n",
        "print(\"\\nImplementing unified delay field creation...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ta7D8Ts1sCQ6",
        "outputId": "956f3dd3-5eb4-4d59-99a5-a0043f22c6e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Implementing unified delay field creation...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "df_consistent = df_imputed.withColumn(\n",
        "    \"unified_delay\",\n",
        "    F.coalesce(\n",
        "        col(\"delay_minutes\"),\n",
        "        F.round(col(\"Delay Logs\")).cast(IntegerType())\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "cjV7jyAAsUAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create delay categories for analysis\n",
        "df_consistent = df_consistent.withColumn(\n",
        "    \"delay_category\",\n",
        "    when(col(\"unified_delay\") < -2, \"Early\")\n",
        "    .when((col(\"unified_delay\") >= -2) & (col(\"unified_delay\") <= 2), \"On-Time\")\n",
        "    .when((col(\"unified_delay\") > 2) & (col(\"unified_delay\") <= 10), \"Minor Delay\")\n",
        "    .when(col(\"unified_delay\") > 10, \"Major Delay\")\n",
        "    .otherwise(\"Unknown\")\n",
        ")\n"
      ],
      "metadata": {
        "id": "5gwKKAh7sftk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show delay distribution\n",
        "print(\"\\nDelay category distribution:\")\n",
        "df_consistent.groupBy(\"delay_category\").count().orderBy(\"count\", ascending=False).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvsJZAyuslZ9",
        "outputId": "d78a84c4-742e-489e-e6ac-fe22d19a2a15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Delay category distribution:\n",
            "+--------------+-----+\n",
            "|delay_category|count|\n",
            "+--------------+-----+\n",
            "|   Minor Delay|15063|\n",
            "|   Major Delay| 9305|\n",
            "|       On-Time| 5632|\n",
            "+--------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PREPROCESSING TECHNIQUE 3: OUTLIER DETECTION AND TREATMENT"
      ],
      "metadata": {
        "id": "SSb-829Jswvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== PREPROCESSING TECHNIQUE 3: OUTLIER DETECTION AND TREATMENT ===\")\n",
        "\n",
        "print(\"\\nJustification:\")\n",
        "print(\"- Identified extreme passenger counts (300 records with invalid values)\")\n",
        "print(\"- Unrealistic fare amounts affecting revenue analysis\")\n",
        "print(\"- Outliers can skew statistical models and lead to incorrect insights\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBCwVD0FsxtX",
        "outputId": "a2f519bc-cb20-4c7e-81d8-657e0985277f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== PREPROCESSING TECHNIQUE 3: OUTLIER DETECTION AND TREATMENT ===\n",
            "\n",
            "Justification:\n",
            "- Identified extreme passenger counts (300 records with invalid values)\n",
            "- Unrealistic fare amounts affecting revenue analysis\n",
            "- Outliers can skew statistical models and lead to incorrect insights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate outlier boundaries using IQR method\n",
        "print(\"\\nCalculating outlier boundaries using IQR method...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VfbvPWvtB7x",
        "outputId": "c861489e-caf0-44a8-9da8-37f2e2a2bd00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculating outlier boundaries using IQR method...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_outlier_bounds(df, column):\n",
        "    \"\"\"Calculate outlier bounds using IQR method\"\"\"\n",
        "    quantiles = df.select(\n",
        "        percentile_approx(column, 0.25).alias(\"q1\"),\n",
        "        percentile_approx(column, 0.75).alias(\"q3\")\n",
        "    ).collect()[0]\n",
        "\n",
        "    q1, q3 = quantiles[\"q1\"], quantiles[\"q3\"]\n",
        "    iqr = q3 - q1\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "\n",
        "    return lower_bound, upper_bound, q1, q3\n"
      ],
      "metadata": {
        "id": "Jpy-N2A7tFfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate bounds for passenger count\n",
        "passenger_bounds = calculate_outlier_bounds(df_consistent, \"Passenger Count\")\n",
        "print(f\"Passenger Count bounds: Lower={passenger_bounds[0]:.2f}, Upper={passenger_bounds[1]:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1MsC_MttKjQ",
        "outputId": "59ee7196-2f12-4561-9b87-d7b731a79e70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passenger Count bounds: Lower=2.00, Upper=18.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate bounds for fare data\n",
        "fare_bounds = calculate_outlier_bounds(df_consistent, \"Fare Data\")\n",
        "print(f\"Fare Data bounds: Lower={fare_bounds[0]:.2f}, Upper={fare_bounds[1]:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jtaaa6cstPz6",
        "outputId": "92cf1a2e-8bd7-48eb-ce71-4a8591166001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fare Data bounds: Lower=-36.75, Upper=146.08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Strategy: Cap outliers instead of removing them (preserves data volume)\n",
        "print(\"\\nImplementing outlier capping...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEt12mdLtUOy",
        "outputId": "f5403523-ee39-49e3-b531-f2373c995829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Implementing outlier capping...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col, lit\n",
        "\n",
        "df_clean = df_consistent.withColumn(\n",
        "    \"Passenger Count\",\n",
        "    F.when(col(\"Passenger Count\") < 0, lit(0))\n",
        "     .when(col(\"Passenger Count\") > passenger_bounds[1], lit(passenger_bounds[1]))\n",
        "     .otherwise(col(\"Passenger Count\"))\n",
        ").withColumn(\n",
        "    \"Fare Data\",\n",
        "    F.when(col(\"Fare Data\") < fare_bounds[0], lit(fare_bounds[0]))\n",
        "     .when(col(\"Fare Data\") > fare_bounds[1], lit(fare_bounds[1]))\n",
        "     .otherwise(col(\"Fare Data\"))\n",
        ")"
      ],
      "metadata": {
        "id": "MpaV4pnut21d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count outliers handled\n",
        "passenger_outliers_before = df_consistent.filter(\n",
        "    (col(\"Passenger Count\") < 0) | (col(\"Passenger Count\") > passenger_bounds[1])\n",
        ").count()\n",
        "\n",
        "fare_outliers_before = df_consistent.filter(\n",
        "    (col(\"Fare Data\") < fare_bounds[0]) | (col(\"Fare Data\") > fare_bounds[1])\n",
        ").count()\n",
        "\n",
        "print(f\"Passenger outliers handled: {passenger_outliers_before}\")\n",
        "print(f\"Fare outliers handled: {fare_outliers_before}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvUX6lGjt5C1",
        "outputId": "caa9510d-5366-4f36-f661-957f1745e3dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passenger outliers handled: 514\n",
            "Fare outliers handled: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FEATURE ENGINEERING AND DATA TRANSFORMATION"
      ],
      "metadata": {
        "id": "NA64hL6wuEko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== FEATURE ENGINEERING AND DATA TRANSFORMATION ===\")\n",
        "\n",
        "print(\"\\nJustification:\")\n",
        "print(\"- Raw timestamp data needs temporal features for time-based analysis\")\n",
        "print(\"- Geographic coordinates need distance calculations for route optimization\")\n",
        "print(\"- Revenue and efficiency metrics needed for business intelligence\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzD36QSxuFf_",
        "outputId": "88356786-df66-413f-9787-b9f46dddf917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== FEATURE ENGINEERING AND DATA TRANSFORMATION ===\n",
            "\n",
            "Justification:\n",
            "- Raw timestamp data needs temporal features for time-based analysis\n",
            "- Geographic coordinates need distance calculations for route optimization\n",
            "- Revenue and efficiency metrics needed for business intelligence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import hour, dayofweek, month, year, when, col\n",
        "\n",
        "print(\"\\nExtracting temporal features...\")\n",
        "df_featured = df_clean.withColumn(\"hour_of_day\", hour(\"Timestamp\")) \\\n",
        "    .withColumn(\"day_of_week\", dayofweek(\"Timestamp\")) \\\n",
        "    .withColumn(\"month\", month(\"Timestamp\")) \\\n",
        "    .withColumn(\"year\", year(\"Timestamp\")) \\\n",
        "    .withColumn(\n",
        "        \"time_period\",\n",
        "        when((col(\"hour_of_day\") >= 6) & (col(\"hour_of_day\") < 10), \"Morning Rush\")\n",
        "        .when((col(\"hour_of_day\") >= 10) & (col(\"hour_of_day\") < 16), \"Midday\")\n",
        "        .when((col(\"hour_of_day\") >= 16) & (col(\"hour_of_day\") < 20), \"Evening Rush\")\n",
        "        .otherwise(\"Off-Peak\")\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvnrsuoYuUTw",
        "outputId": "d9c84863-61f6-45fa-bdb7-281e1a9e1f79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracting temporal features...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Revenue and efficiency metrics\n",
        "print(\"Creating business intelligence features...\")\n",
        "df_featured = df_featured.withColumn(\n",
        "    \"revenue_per_passenger\",\n",
        "    when(col(\"Passenger Count\") > 0, col(\"Fare Data\") / col(\"Passenger Count\"))\n",
        "    .otherwise(lit(0))\n",
        ").withColumn(\n",
        "    \"efficiency_score\",\n",
        "    when(col(\"unified_delay\") <= 2, col(\"Passenger Count\") * 1.2)  # Bonus for on-time\n",
        "    .otherwise(col(\"Passenger Count\") * (1 - (col(\"unified_delay\") * 0.1)))  # Penalty for delays\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4On3_F4uWpa",
        "outputId": "83b51bef-ceb6-498a-8166-e6088c415780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating business intelligence features...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf, col\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Define the function\n",
        "def classify_peak_hour(hour):\n",
        "    \"\"\"Classify hour as peak or off-peak\"\"\"\n",
        "    if 7 <= hour <= 9 or 17 <= hour <= 19:\n",
        "        return \"Peak\"\n",
        "    else:\n",
        "        return \"Off-Peak\"\n",
        "\n",
        "# Register as UDF\n",
        "classify_peak_hour_udf = udf(classify_peak_hour, StringType())\n",
        "\n",
        "# Use in DataFrame\n",
        "df_featured = df_featured.withColumn(\"peak_indicator\", classify_peak_hour_udf(col(\"hour_of_day\")))"
      ],
      "metadata": {
        "id": "R87UXx7nujhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXPLORATORY DATA ANALYSIS"
      ],
      "metadata": {
        "id": "4nY1pbNStI8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== EXPLORATORY DATA ANALYSIS ===\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AnYCnP7upWa",
        "outputId": "e5323f31-607f-4594-fd34-6119e617983e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EXPLORATORY DATA ANALYSIS ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic statistics after preprocessing\n",
        "print(\"\\nPost-preprocessing dataset statistics:\")\n",
        "df_featured.describe().show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_fFLtSNuxU8",
        "outputId": "b8e349f6-b168-47f3-e844-ea9a2347c25d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Post-preprocessing dataset statistics:\n",
            "+-------+--------+-------+------------------+------------------+-------------------+------------------+------------------+-----------------+------------------+-----------------+--------------+------------------+------------------+-----------------+--------------------+------------+---------------------+------------------+--------------+\n",
            "|summary|Route ID|Stop ID|   Passenger Count|          Latitude|          Longitude|        Delay Logs|         Fare Data|    delay_minutes|       fare_amount|    unified_delay|delay_category|       hour_of_day|       day_of_week|            month|                year| time_period|revenue_per_passenger|  efficiency_score|peak_indicator|\n",
            "+-------+--------+-------+------------------+------------------+-------------------+------------------+------------------+-----------------+------------------+-----------------+--------------+------------------+------------------+-----------------+--------------------+------------+---------------------+------------------+--------------+\n",
            "|  count|   30000|  30000|             28516|              1499|              28500|             28514|             28516|            30000|             30000|            30000|         30000|             30000|             30000|            30000|               30000|       30000|                28598|             28516|         30000|\n",
            "|   mean|    NULL|   NULL|10.065436947678496|28.016482472314916|  85.40057991242071|10.530081714245673| 54.43123790152908|7.478633333333334|18.739316666666667|7.478633333333334|          NULL|11.542933333333334|            4.0054|             6.53|  2025.0006333333333|        NULL|    5.819835939547452|3.1060702763361028|          NULL|\n",
            "| stddev|    NULL|   NULL|3.2245762744806195| 0.290744047739916|0.11553323310202124|102.05187071718382|26.973556621741583|4.612546583514199| 2.306273291757099|4.612546583514199|          NULL| 6.961027600797466|1.9892970492341455|3.450559888692202|0.025158563565242105|        NULL|    4.622261450310626|5.8773275182071005|          NULL|\n",
            "|    min|    R001|   S001|               0.0|         27.502284|          85.196462|            -18.72|             -20.0|                0|              15.0|                0|   Major Delay|                 0|                 1|                1|                2025|Evening Rush|                -10.0|              -9.0|      Off-Peak|\n",
            "|    max|    R020|   S100|              18.0|         28.499515|          85.601999|             999.0|             100.0|               15|              22.5|               15|       On-Time|                23|                 7|               12|                2026|    Off-Peak|                98.65|21.599999999999998|          Peak|\n",
            "+-------+--------+-------+------------------+------------------+-------------------+------------------+------------------+-----------------+------------------+-----------------+--------------+------------------+------------------+-----------------+--------------------+------------+---------------------+------------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Temporal patterns analysis\n",
        "print(\"\\nTemporal patterns analysis:\")\n",
        "print(\"1. Passenger volume by time period:\")\n",
        "df_featured.groupBy(\"time_period\").agg(\n",
        "    {\"Passenger Count\": \"avg\", \"unified_delay\": \"avg\"}\n",
        ").withColumnRenamed(\"avg(Passenger Count)\", \"avg_passengers\") \\\n",
        " .withColumnRenamed(\"avg(unified_delay)\", \"avg_delay\") \\\n",
        " .orderBy(\"avg_passengers\", ascending=False).show()\n",
        "\n",
        "print(\"2. Route performance analysis:\")\n",
        "route_performance = df_featured.groupBy(\"Route ID\").agg(\n",
        "    {\"Passenger Count\": \"sum\", \"Fare Data\": \"sum\", \"unified_delay\": \"avg\"}\n",
        ").withColumnRenamed(\"sum(Passenger Count)\", \"total_passengers\") \\\n",
        " .withColumnRenamed(\"sum(Fare Data)\", \"total_revenue\") \\\n",
        " .withColumnRenamed(\"avg(unified_delay)\", \"avg_delay\")\n",
        "\n",
        "route_performance.show(10)\n",
        "\n",
        "print(\"3. Delay pattern analysis:\")\n",
        "df_featured.groupBy(\"delay_category\").count().orderBy(\"count\", ascending=False).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQq877iAvI9m",
        "outputId": "cd31cb1f-41bd-4624-d076-6acc9271c403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Temporal patterns analysis:\n",
            "1. Passenger volume by time period:\n",
            "+------------+------------------+-----------------+\n",
            "| time_period|    avg_passengers|        avg_delay|\n",
            "+------------+------------------+-----------------+\n",
            "|Evening Rush|10.104008438818566|7.455164131305044|\n",
            "|      Midday|10.100214745884037|7.526750643369904|\n",
            "|    Off-Peak| 10.05142241021813|7.474079342500395|\n",
            "|Morning Rush| 10.01119560625264|7.442319307429032|\n",
            "+------------+------------------+-----------------+\n",
            "\n",
            "2. Route performance analysis:\n",
            "+--------+-----------------+----------------+------------------+\n",
            "|Route ID|    total_revenue|total_passengers|         avg_delay|\n",
            "+--------+-----------------+----------------+------------------+\n",
            "|    R018|76722.59999999999|         13932.0|7.4890260631001375|\n",
            "|    R014|79612.09999999992|         14562.0| 7.509114583333333|\n",
            "|    R007|79419.91000000012|         14404.0| 7.517857142857143|\n",
            "|    R005|80362.92000000004|         14852.0| 7.776493256262042|\n",
            "|    R017|74345.04999999997|         13786.0| 7.676304654442878|\n",
            "|    R001|78356.07000000002|         14853.0| 7.356867779204108|\n",
            "|    R010| 75285.2199999999|         14044.0| 7.361950549450549|\n",
            "|    R004|77760.27999999997|         14091.0| 7.545516769336071|\n",
            "|    R013|79895.56000000008|         15031.0| 7.427476038338658|\n",
            "|    R019|76612.28000000019|         14369.0| 7.495652173913044|\n",
            "+--------+-----------------+----------------+------------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "3. Delay pattern analysis:\n",
            "+--------------+-----+\n",
            "|delay_category|count|\n",
            "+--------------+-----+\n",
            "|   Minor Delay|15063|\n",
            "|   Major Delay| 9305|\n",
            "|       On-Time| 5632|\n",
            "+--------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display final schema\n",
        "print(\"\\nFinal dataset schema:\")\n",
        "df_featured.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4jjLahqvc3I",
        "outputId": "c73883c8-c959-4649-e342-e1851e99a251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final dataset schema:\n",
            "root\n",
            " |-- Route ID: string (nullable = true)\n",
            " |-- Stop ID: string (nullable = true)\n",
            " |-- Timestamp: timestamp (nullable = true)\n",
            " |-- Passenger Count: double (nullable = true)\n",
            " |-- Latitude: double (nullable = true)\n",
            " |-- Longitude: double (nullable = true)\n",
            " |-- Delay Logs: double (nullable = true)\n",
            " |-- Fare Data: double (nullable = true)\n",
            " |-- delay_minutes: integer (nullable = true)\n",
            " |-- fare_amount: double (nullable = true)\n",
            " |-- unified_delay: integer (nullable = true)\n",
            " |-- delay_category: string (nullable = false)\n",
            " |-- hour_of_day: integer (nullable = true)\n",
            " |-- day_of_week: integer (nullable = true)\n",
            " |-- month: integer (nullable = true)\n",
            " |-- year: integer (nullable = true)\n",
            " |-- time_period: string (nullable = false)\n",
            " |-- revenue_per_passenger: double (nullable = true)\n",
            " |-- efficiency_score: double (nullable = true)\n",
            " |-- peak_indicator: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nFinal dataset: {df_featured.count()} rows, {len(df_featured.columns)} columns\")\n",
        "print(\"Preprocessing pipeline completed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIpVXA2SvkAx",
        "outputId": "a1851849-be37-4a53-87fd-bbbeb521c4ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final dataset: 30000 rows, 20 columns\n",
            "Preprocessing pipeline completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Spark** Operations Comparison"
      ],
      "metadata": {
        "id": "OkkKbC6xwKLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import (\n",
        "    col, when, avg, sum as spark_sum, count, max as spark_max, min as spark_min,\n",
        "    hour, dayofweek, stddev, percentile_approx, desc, asc,\n",
        "    lag, lead, window, collect_list, expr, round as spark_round\n",
        ")\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "import time\n",
        "import psutil\n",
        "import os"
      ],
      "metadata": {
        "id": "Q3wnEo8pw0mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark Session with performance monitoring\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SmartCitySparkComparison\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "1vAYB1fUxfku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set log level to reduce noise\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "print(\"=== SPARK OPERATIONS COMPARISON FOR SMART CITY TRANSPORT ===\")\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"Available Cores: {spark.sparkContext.defaultParallelism}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8caSOkq6xj_-",
        "outputId": "2463e912-7443-404b-8409-6bd0e63563f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== SPARK OPERATIONS COMPARISON FOR SMART CITY TRANSPORT ===\n",
            "Spark Version: 3.5.1\n",
            "Available Cores: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load preprocessed dataset\n",
        "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\") \\\n",
        "    .csv(\"/content/drive/MyDrive/Big_Data_public_transport_dataset_v2.csv\")"
      ],
      "metadata": {
        "id": "KCzz1UIExpf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic preprocessing for consistency\n",
        "df = df.fillna({\n",
        "    \"Passenger Count\": 10,\n",
        "    \"Fare Data\": 45.0,\n",
        "    \"Delay Logs\": 0.0,\n",
        "    \"delay_minutes\": 0\n",
        "}).withColumn(\"unified_delay\", col(\"delay_minutes\")) \\\n",
        "  .withColumn(\"hour_of_day\", hour(\"Timestamp\")) \\\n",
        "  .withColumn(\"day_of_week\", dayofweek(\"Timestamp\"))\n",
        "\n",
        "print(f\"Dataset loaded: {df.count()} rows, {len(df.columns)} columns\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEtrW_Yax3z5",
        "outputId": "6b07da8c-f1b5-440f-bc3d-b3beaaa09f13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded: 30000 rows, 13 columns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Performance monitoring utility\n",
        "class PerformanceMonitor:\n",
        "    def __init__(self, operation_name):\n",
        "        self.operation_name = operation_name\n",
        "        self.start_time = None\n",
        "        self.start_memory = None\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.start_time = time.time()\n",
        "        self.start_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        end_time = time.time()\n",
        "        end_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024\n",
        "\n",
        "        execution_time = end_time - self.start_time\n",
        "        memory_usage = end_memory - self.start_memory\n",
        "\n",
        "        print(f\"[{self.operation_name}] Execution Time: {execution_time:.3f}s, Memory Delta: {memory_usage:.2f}MB\")\n",
        "        return execution_time, memory_usage"
      ],
      "metadata": {
        "id": "D4bzcCdcx8Fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANALYSIS 1: ROUTE CONGESTION/TRAVEL TIME PREDICTION"
      ],
      "metadata": {
        "id": "ZyDQwbz_yBX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANALYSIS 1: ROUTE CONGESTION/TRAVEL TIME PREDICTION\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mh846_CyIWa",
        "outputId": "ea36beb7-9de2-4b0f-cfb4-3a1c2649bcfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ANALYSIS 1: ROUTE CONGESTION/TRAVEL TIME PREDICTION\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create congestion indicator based on passenger density and delays\n",
        "df_congestion = df.withColumn(\n",
        "    \"congestion_score\",\n",
        "    (col(\"Passenger Count\") * 0.6 + col(\"unified_delay\") * 0.4)\n",
        ").withColumn(\n",
        "    \"congestion_level\",\n",
        "    when(col(\"congestion_score\") <= 8, \"Low\")\n",
        "    .when((col(\"congestion_score\") > 8) & (col(\"congestion_score\") <= 15), \"Medium\")\n",
        "    .otherwise(\"High\")\n",
        ")"
      ],
      "metadata": {
        "id": "vF9qbzrEzE21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cache for performance comparison\n",
        "df_congestion.cache()\n",
        "\n",
        "print(\"\\n--- RDD APPROACH: Route Congestion Analysis ---\")\n",
        "with PerformanceMonitor(\"RDD Congestion Analysis\") as rdd_perf:\n",
        "    # Convert to RDD and perform operations\n",
        "    rdd_data = df_congestion.rdd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RXYEAVjzISu",
        "outputId": "182ae344-c082-4110-8184-ebeabb10debf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- RDD APPROACH: Route Congestion Analysis ---\n",
            "[RDD Congestion Analysis] Execution Time: 1.537s, Memory Delta: 0.20MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "congestion_rdd = rdd_data.map(lambda row: (\n",
        "    row['Route ID'],\n",
        "    (row['congestion_score'], row['Passenger Count'], row['unified_delay'], 1)\n",
        "))"
      ],
      "metadata": {
        "id": "9jtkFdo3zhew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce by route to get aggregated metrics\n",
        "route_congestion_rdd = congestion_rdd.reduceByKey(lambda a, b: (\n",
        "    a[0] + b[0],  # Sum congestion scores\n",
        "    a[1] + b[1],  # Sum passengers\n",
        "    a[2] + b[2],  # Sum delays\n",
        "    a[3] + b[3]   # Count records\n",
        "))"
      ],
      "metadata": {
        "id": "b0N6TLw1ztnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "route_congestion_results_rdd = route_congestion_rdd.map(lambda x: {\n",
        "    'route': x[0],\n",
        "    'avg_congestion': x[1][0] / x[1][3] if x[1][3] != 0 else None,\n",
        "    'total_passengers': x[1][1],\n",
        "    'avg_delay': x[1][2] / x[1][3] if x[1][3] != 0 else None,\n",
        "    'trip_count': x[1][3]\n",
        "}).collect()"
      ],
      "metadata": {
        "id": "aOPnhjN9zvXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort by congestion score\n",
        "sorted_routes = sorted(route_congestion_results_rdd, key=lambda x: x['avg_congestion'], reverse=True)\n",
        "\n",
        "print(\"RDD Results - Top 5 Most Congested Routes:\")\n",
        "for i, route in enumerate(sorted_routes[:5]):\n",
        "    print(f\"{i+1}. Route {route['route']}: Congestion={route['avg_congestion']:.2f}, \"\n",
        "          f\"Passengers={route['total_passengers']}, Avg Delay={route['avg_delay']:.2f}min\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMzIvang0Jic",
        "outputId": "afa59779-1626-46e3-930b-df76c5b233c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDD Results - Top 5 Most Congested Routes:\n",
            "1. Route R018: Congestion=17.50, Passengers=35247.0, Avg Delay=7.49min\n",
            "2. Route R019: Congestion=17.37, Passengers=35817.0, Avg Delay=7.50min\n",
            "3. Route R013: Congestion=16.19, Passengers=34468.0, Avg Delay=7.43min\n",
            "4. Route R020: Congestion=15.91, Passengers=31645.0, Avg Delay=7.48min\n",
            "5. Route R003: Congestion=15.87, Passengers=33119.0, Avg Delay=7.39min\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use DataFrame operations\n",
        "route_congestion_df = df_congestion.groupBy(\"Route ID\").agg(\n",
        "    avg(\"congestion_score\").alias(\"avg_congestion\"),\n",
        "    spark_sum(\"Passenger Count\").alias(\"total_passengers\"),\n",
        "    avg(\"unified_delay\").alias(\"avg_delay\"),\n",
        "    count(\"*\").alias(\"trip_count\")\n",
        ").orderBy(desc(\"avg_congestion\"))\n",
        "\n",
        "route_congestion_results_df = route_congestion_df.collect()\n",
        "\n",
        "print(\"DataFrame Results - Top 5 Most Congested Routes:\")\n",
        "for i, row in enumerate(route_congestion_results_df[:5]):\n",
        "    print(f\"{i+1}. Route {row['Route ID']}: Congestion={row['avg_congestion']:.2f}, \"\n",
        "          f\"Passengers={row['total_passengers']}, Avg Delay={row['avg_delay']:.2f}min\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5mHLwr40fn2",
        "outputId": "dda085cb-92d0-46cf-c1dc-d1c022141976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame Results - Top 5 Most Congested Routes:\n",
            "1. Route R018: Congestion=17.50, Passengers=35247.0, Avg Delay=7.49min\n",
            "2. Route R019: Congestion=17.37, Passengers=35817.0, Avg Delay=7.50min\n",
            "3. Route R013: Congestion=16.19, Passengers=34468.0, Avg Delay=7.43min\n",
            "4. Route R020: Congestion=15.91, Passengers=31645.0, Avg Delay=7.48min\n",
            "5. Route R003: Congestion=15.87, Passengers=33119.0, Avg Delay=7.39min\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"your_app_name\").getOrCreate()"
      ],
      "metadata": {
        "id": "YXnbxecq0t_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SQL query for congestion analysis\n",
        "congestion_sql = \"\"\"\n",
        "SELECT\n",
        "    `Route ID`,\n",
        "    AVG(congestion_score) as avg_congestion,\n",
        "    SUM(`Passenger Count`) as total_passengers,\n",
        "    AVG(unified_delay) as avg_delay,\n",
        "    COUNT(*) as trip_count,\n",
        "    PERCENTILE_APPROX(congestion_score, 0.9) as congestion_90th_percentile\n",
        "FROM transport_data\n",
        "GROUP BY `Route ID`\n",
        "ORDER BY avg_congestion DESC\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "6HqyOYdi1KsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Create sample data\n",
        "data = [\n",
        "    (\"R1\", 1.5, 20, 5.0),\n",
        "    (\"R1\", 2.0, 25, 7.0),\n",
        "    (\"R2\", 3.0, 30, 10.0),\n",
        "    (\"R2\", 3.5, 35, 12.0),\n",
        "    (\"R3\", 1.0, 15, 3.0)\n",
        "]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"Route ID\", StringType(), True),\n",
        "    StructField(\"congestion_score\", DoubleType(), True),\n",
        "    StructField(\"Passenger Count\", IntegerType(), True),\n",
        "    StructField(\"unified_delay\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.createOrReplaceTempView(\"transport_data\")"
      ],
      "metadata": {
        "id": "8cen9rzB1w3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- SQL APPROACH: Route Congestion Analysis ---\")\n",
        "with PerformanceMonitor(\"SQL Congestion Analysis\") as sql_perf:\n",
        "    # Register as temporary view\n",
        "    df_congestion.createOrReplaceTempView(\"transport_data\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTuEWfoO4rgq",
        "outputId": "688231a8-1f6f-457f-862c-a43267dad4cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- SQL APPROACH: Route Congestion Analysis ---\n",
            "[SQL Congestion Analysis] Execution Time: 0.056s, Memory Delta: 0.00MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANALYSIS 2: PUBLIC TRANSPORT RESOURCE UTILIZATION"
      ],
      "metadata": {
        "id": "qw7fe_HG44t5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANALYSIS 2: PUBLIC TRANSPORT RESOURCE UTILIZATION\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSHpCMVQ44b0",
        "outputId": "86e9a258-7cbe-404b-9fca-ad3e4f643d2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ANALYSIS 2: PUBLIC TRANSPORT RESOURCE UTILIZATION\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- RDD APPROACH: Resource Utilization Analysis ---\")\n",
        "with PerformanceMonitor(\"RDD Resource Utilization\") as rdd_util_perf:\n",
        "    # Calculate utilization metrics using RDD\n",
        "    utilization_rdd = df_congestion.rdd.map(lambda row: (\n",
        "        (row['Route ID'], row['hour_of_day']),\n",
        "        (row['Passenger Count'], row['Fare Data'], 1)\n",
        "    ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KDyEAoe5Cp2",
        "outputId": "7a9e0255-9243-4cf1-a544-1616699aadf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- RDD APPROACH: Resource Utilization Analysis ---\n",
            "[RDD Resource Utilization] Execution Time: 0.002s, Memory Delta: 0.00MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # Aggregate by route and hour\n",
        "    hourly_utilization_rdd = utilization_rdd.reduceByKey(lambda a, b: (\n",
        "        a[0] + b[0],  # Sum passengers\n",
        "        a[1] + b[1],  # Sum revenue\n",
        "        a[2] + b[2]   # Count trips\n",
        "    ))"
      ],
      "metadata": {
        "id": "MX6tZxOw5G-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate efficiency metrics with safeguards for division by zero\n",
        "efficiency_rdd = hourly_utilization_rdd.map(lambda x: {\n",
        "    'route': x[0][0],\n",
        "    'hour': x[0][1],\n",
        "    'passengers_per_trip': x[1][0] / x[1][2] if x[1][2] > 0 else 0,\n",
        "    'revenue_per_trip': x[1][1] / x[1][2] if x[1][2] > 0 else 0,\n",
        "    'total_trips': x[1][2]\n",
        "})"
      ],
      "metadata": {
        "id": "8llDtWJ-5ZIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find peak utilization hours\n",
        "peak_hours_rdd = (efficiency_rdd\n",
        "                  .filter(lambda x: x['passengers_per_trip'] > 12)\n",
        "                  .sortBy(lambda x: x['passengers_per_trip'], ascending=False)\n",
        "                  .collect())"
      ],
      "metadata": {
        "id": "ftD2FYfE5t7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"RDD Results - Top 5 Peak Utilization Periods:\")\n",
        "for i, period in enumerate(peak_hours_rdd[:5]):\n",
        "    print(f\"{i+1}. Route {period['route']} at {period['hour']}:00 - \"\n",
        "          f\"{period['passengers_per_trip']:.1f} passengers/trip, \"\n",
        "          f\"${period['revenue_per_trip']:.2f}/trip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXnni8pU6BY7",
        "outputId": "c32fceec-db52-4a70-9204-fb8c8a57c70a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDD Results - Top 5 Peak Utilization Periods:\n",
            "1. Route R012 at 7:00 - 73.2 passengers/trip, $50.43/trip\n",
            "2. Route R013 at 0:00 - 62.5 passengers/trip, $56.97/trip\n",
            "3. Route R004 at 14:00 - 61.0 passengers/trip, $57.61/trip\n",
            "4. Route R003 at 9:00 - 57.8 passengers/trip, $59.05/trip\n",
            "5. Route R008 at 10:00 - 57.5 passengers/trip, $55.19/trip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg, sum, count, col, desc\n",
        "\n",
        "# DataFrame operations for utilization\n",
        "utilization_df = df_congestion.groupBy(\"Route ID\", \"hour_of_day\").agg(\n",
        "    avg(\"Passenger Count\").alias(\"avg_passengers_per_trip\"),\n",
        "    avg(\"Fare Data\").alias(\"avg_revenue_per_trip\"),\n",
        "    count(\"*\").alias(\"total_trips\"),\n",
        "    sum(\"Passenger Count\").alias(\"total_passengers\")\n",
        ").withColumn(\n",
        "    \"utilization_efficiency\",\n",
        "    col(\"avg_passengers_per_trip\") * col(\"avg_revenue_per_trip\") / 100\n",
        ").orderBy(desc(\"avg_passengers_per_trip\"))"
      ],
      "metadata": {
        "id": "1K8ac0XR6e-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "utilization_results_df = utilization_df.filter(col(\"avg_passengers_per_trip\") > 12).collect()\n"
      ],
      "metadata": {
        "id": "wqSIz84v6hMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"DataFrame Results - Top 5 Peak Utilization Periods:\")\n",
        "for i, row in enumerate(utilization_results_df[:5]):\n",
        "    print(f\"{i+1}. Route {row['Route ID']} at {row['hour_of_day']}:00 - \"\n",
        "          f\"{row['avg_passengers_per_trip']:.1f} passengers/trip, \"\n",
        "          f\"${row['avg_revenue_per_trip']:.2f}/trip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--_cx9ty6mh2",
        "outputId": "6dd4b417-267b-4bf6-92cc-974ce969c05e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame Results - Top 5 Peak Utilization Periods:\n",
            "1. Route R012 at 7:00 - 73.2 passengers/trip, $50.43/trip\n",
            "2. Route R013 at 0:00 - 62.5 passengers/trip, $56.97/trip\n",
            "3. Route R004 at 14:00 - 61.0 passengers/trip, $57.61/trip\n",
            "4. Route R003 at 9:00 - 57.8 passengers/trip, $59.05/trip\n",
            "5. Route R008 at 10:00 - 57.5 passengers/trip, $55.19/trip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- SQL APPROACH: Resource Utilization Analysis ---\")\n",
        "with PerformanceMonitor(\"SQL Resource Utilization\") as sql_util_perf:\n",
        "    utilization_sql = \"\"\"\n",
        "    SELECT\n",
        "        `Route ID`,\n",
        "        hour_of_day,\n",
        "        AVG(`Passenger Count`) as avg_passengers_per_trip,\n",
        "        AVG(`Fare Data`) as avg_revenue_per_trip,\n",
        "        COUNT(*) as total_trips,\n",
        "        SUM(`Passenger Count`) as total_passengers,\n",
        "        (AVG(`Passenger Count`) * AVG(`Fare Data`)) / 100 as utilization_efficiency\n",
        "    FROM transport_data\n",
        "    GROUP BY `Route ID`, hour_of_day\n",
        "    HAVING AVG(`Passenger Count`) > 12\n",
        "    ORDER BY avg_passengers_per_trip DESC\n",
        "    \"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dT_aADxZ6rWQ",
        "outputId": "3e9e70db-205d-4840-c1a9-57a2c3e16d05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- SQL APPROACH: Resource Utilization Analysis ---\n",
            "[SQL Resource Utilization] Execution Time: 0.001s, Memory Delta: 0.00MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "utilization_sql_results = spark.sql(utilization_sql).collect()"
      ],
      "metadata": {
        "id": "Gsg-MEbr6wEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"SQL Results - Top 5 Peak Utilization Periods:\")\n",
        "for i, row in enumerate(utilization_sql_results[:5]):\n",
        "    print(f\"{i+1}. Route {row['Route ID']} at {row['hour_of_day']}:00 - \"\n",
        "          f\"{row['avg_passengers_per_trip']:.1f} passengers/trip, \"\n",
        "          f\"Efficiency Score: {row['utilization_efficiency']:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9jlkF9k7BzC",
        "outputId": "35696c27-42df-4985-ed1e-884be92d27b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SQL Results - Top 5 Peak Utilization Periods:\n",
            "1. Route R012 at 7:00 - 73.2 passengers/trip, Efficiency Score: 36.90\n",
            "2. Route R013 at 0:00 - 62.5 passengers/trip, Efficiency Score: 35.59\n",
            "3. Route R004 at 14:00 - 61.0 passengers/trip, Efficiency Score: 35.14\n",
            "4. Route R003 at 9:00 - 57.8 passengers/trip, Efficiency Score: 34.12\n",
            "5. Route R008 at 10:00 - 57.5 passengers/trip, Efficiency Score: 31.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANALYSIS 3: DELAY EFFECTIVENESS ANALYSIS"
      ],
      "metadata": {
        "id": "PKPrwutq7Gz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANALYSIS 3: DELAY EFFECTIVENESS ANALYSIS\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWcnRy7B7KBc",
        "outputId": "deba9ba2-c6d8-4f9c-e956-721dba9f73bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ANALYSIS 3: DELAY EFFECTIVENESS ANALYSIS\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- RDD APPROACH: Delay Effectiveness Analysis ---\")\n",
        "with PerformanceMonitor(\"RDD Delay Analysis\") as rdd_delay_perf:\n",
        "    # Analyze delay patterns and their impact\n",
        "    delay_impact_rdd = df_congestion.rdd.map(lambda row: (\n",
        "        row['Route ID'],\n",
        "        (row['unified_delay'], row['Passenger Count'], row['Fare Data'], 1)\n",
        "    ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FL_IX-PH7ROE",
        "outputId": "1779d802-0b0a-42fd-8c94-b4905e172f48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- RDD APPROACH: Delay Effectiveness Analysis ---\n",
            "[RDD Delay Analysis] Execution Time: 0.002s, Memory Delta: 0.00MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate total delay impact by route\n",
        "route_delay_rdd = delay_impact_rdd.reduceByKey(\n",
        "    lambda a, b: (\n",
        "        a[0] + b[0],  # Sum delays\n",
        "        a[1] + b[1],  # Sum passengers\n",
        "        a[2] + b[2],  # Sum revenue\n",
        "        a[3] + b[3]   # Count trips\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "1E9waYPB7abJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"DelayAnalysis\").getOrCreate()"
      ],
      "metadata": {
        "id": "Z1FrBbrqmadI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example data: (route, [total_delay, passengers_lost, revenue_loss, total_trips])\n",
        "data = [\n",
        "    (\"route1\", [120, 50, 2000, 20]),\n",
        "    (\"route2\", [0, 0, 0, 10]),\n",
        "    (\"route3\", [300, 80, 4000, 25]),\n",
        "    # Add more data as needed.\n",
        "]"
      ],
      "metadata": {
        "id": "-zlzSNcwnTf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "route_delay_rdd = spark.sparkContext.parallelize(data)"
      ],
      "metadata": {
        "id": "uO-rYUJCnXE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(x):\n",
        "    route, metrics = x\n",
        "    total_delay, passengers_lost, revenue_loss, total_trips = metrics\n",
        "    if total_trips == 0:\n",
        "        return None\n",
        "    avg_delay = total_delay / total_trips\n",
        "    if avg_delay > 0:\n",
        "        passengers_lost_per_delay_min = (passengers_lost / total_trips) * avg_delay * 0.05\n",
        "    else:\n",
        "        passengers_lost_per_delay_min = 0\n",
        "    revenue_impact = revenue_loss / total_trips\n",
        "    return {\n",
        "        'route': route,\n",
        "        'avg_delay': avg_delay,\n",
        "        'passengers_lost_per_delay_min': passengers_lost_per_delay_min,\n",
        "        'revenue_impact': revenue_impact,\n",
        "        'total_trips': total_trips\n",
        "    }\n",
        "\n",
        "delay_effectiveness_rdd = route_delay_rdd \\\n",
        "    .map(compute_metrics) \\\n",
        "    .filter(lambda x: x is not None and x['avg_delay'] > 0) \\\n",
        "    .sortBy(lambda x: x['passengers_lost_per_delay_min'], ascending=False) \\\n",
        "    .collect()"
      ],
      "metadata": {
        "id": "AeDHCfRTnYn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"DelayAnalysis\").getOrCreate()\n",
        "\n",
        "# Sample data: (route, [total_delay, passengers_lost, revenue_loss, total_trips])\n",
        "data = [\n",
        "    (\"route1\", [120, 50, 2000, 20]),\n",
        "    (\"route2\", [0, 0, 0, 10]),\n",
        "    (\"route3\", [300, 80, 4000, 25]),\n",
        "    (\"route4\", [150, 60, 3000, 0]),  # edge case: zero trips\n",
        "    (\"route5\", [200, 70, 3500, 25]),\n",
        "    (\"route6\", [180, 65, 3300, 22]),\n",
        "]\n",
        "\n",
        "# Create RDD\n",
        "route_delay_rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "# Define function to compute metrics\n",
        "def compute_metrics(x):\n",
        "    route, metrics = x\n",
        "    total_delay, passengers_lost, revenue_loss, total_trips = metrics\n",
        "    if total_trips == 0:\n",
        "        return None  # skip invalid data\n",
        "    avg_delay = total_delay / total_trips\n",
        "    if avg_delay > 0:\n",
        "        passengers_lost_per_delay_min = (passengers_lost / total_trips) * avg_delay * 0.05\n",
        "    else:\n",
        "        passengers_lost_per_delay_min = 0\n",
        "    revenue_impact = revenue_loss / total_trips\n",
        "    return {\n",
        "        'route': route,\n",
        "        'avg_delay': avg_delay,\n",
        "        'passengers_lost_per_delay_min': passengers_lost_per_delay_min,\n",
        "        'revenue_impact': revenue_impact,\n",
        "        'total_trips': total_trips\n",
        "    }\n",
        "\n",
        "# Process RDD\n",
        "delay_effectiveness_rdd = route_delay_rdd \\\n",
        "    .map(compute_metrics) \\\n",
        "    .filter(lambda x: x is not None and x['avg_delay'] > 0) \\\n",
        "    .sortBy(lambda x: x['passengers_lost_per_delay_min'], ascending=False) \\\n",
        "    .collect()\n",
        "\n",
        "# Print top 5 routes most affected by delays\n",
        "print(\"RDD Results - Routes Most Affected by Delays:\")\n",
        "for i, route in enumerate(delay_effectiveness_rdd[:5]):\n",
        "    print(f\"{i+1}. Route {route['route']}: Avg Delay={route['avg_delay']:.2f} min, \"\n",
        "          f\"Passenger Impact={route['passengers_lost_per_delay_min']:.2f}, \"\n",
        "          f\"Revenue=${route['revenue_impact']:.2f}\")\n",
        "\n",
        "# Placeholder for DataFrame analysis\n",
        "print(\"\\n--- DataFrame APPROACH: Delay Effectiveness Analysis ---\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlgkpGKdny7y",
        "outputId": "0dca8f06-93c0-4fa5-8b1e-b96fcc839a0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDD Results - Routes Most Affected by Delays:\n",
            "1. Route route3: Avg Delay=12.00 min, Passenger Impact=1.92, Revenue=$160.00\n",
            "2. Route route6: Avg Delay=8.18 min, Passenger Impact=1.21, Revenue=$150.00\n",
            "3. Route route5: Avg Delay=8.00 min, Passenger Impact=1.12, Revenue=$140.00\n",
            "4. Route route1: Avg Delay=6.00 min, Passenger Impact=0.75, Revenue=$100.00\n",
            "\n",
            "--- DataFrame APPROACH: Delay Effectiveness Analysis ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "spark = SparkSession.builder.appName(\"DelayCategories\").getOrCreate()\n",
        "\n",
        "# Sample data: list of tuples with delay values\n",
        "data = [\n",
        "    (1, 0),\n",
        "    (2, 3),\n",
        "    (3, 10),\n",
        "    (4, 20)\n",
        "]\n",
        "\n",
        "# Define schema\n",
        "columns = [\"id\", \"unified_delay\"]\n",
        "\n",
        "# Create DataFrame\n",
        "df_congestion = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "# Now apply the category creation\n",
        "from pyspark.sql.functions import when, col\n",
        "\n",
        "delay_analysis_df = df_congestion.withColumn(\n",
        "    \"delay_category\",\n",
        "    when(col(\"unified_delay\") <= 0, \"On-time/Early\")\n",
        "    .when((col(\"unified_delay\") > 0) & (col(\"unified_delay\") <= 5), \"Minor Delay\")\n",
        "    .when((col(\"unified_delay\") > 5) & (col(\"unified_delay\") <= 15), \"Moderate Delay\")\n",
        "    .otherwise(\"Major Delay\")\n",
        ")\n",
        "\n",
        "delay_analysis_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28sdfyqGoosv",
        "outputId": "615e59e5-4b94-4e6f-82c4-88b3985d9728"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------+--------------+\n",
            "| id|unified_delay|delay_category|\n",
            "+---+-------------+--------------+\n",
            "|  1|            0| On-time/Early|\n",
            "|  2|            3|   Minor Delay|\n",
            "|  3|           10|Moderate Delay|\n",
            "|  4|           20|   Major Delay|\n",
            "+---+-------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "delay_analysis_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GS0dWJmSqMuh",
        "outputId": "71187104-38fa-40b2-ad33-f831f9635702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- unified_delay: long (nullable = true)\n",
            " |-- delay_category: string (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANALYSIS 4: COST ANALYSIS PER ROUTE/VEHICLE TYPE"
      ],
      "metadata": {
        "id": "_pYJjHRYramB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANALYSIS 4: COST ANALYSIS PER ROUTE/VEHICLE TYPE\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doyixpsircuh",
        "outputId": "8958eb61-e68e-4b6d-fea7-e52fb60fed2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ANALYSIS 4: COST ANALYSIS PER ROUTE/VEHICLE TYPE\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import when, col\n",
        "\n",
        "#  Initialize Spark session (if not already initialized)\n",
        "spark = SparkSession.builder.appName(\"TransportationAnalysis\").getOrCreate()\n",
        "\n",
        "\n",
        "# For demonstration, create a sample DataFrame\n",
        "data = [\n",
        "    (\"R001\", 10, 100),\n",
        "    (\"R007\", 15, 150),\n",
        "    (\"R012\", 20, 200),\n",
        "]\n",
        "columns = [\"Route ID\", \"Passenger Count\", \"OtherColumn\"]\n",
        "df_featured = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "\n",
        "df_congestion = df_featured.withColumnRenamed(\"Route ID\", \"route_id\")\n",
        "\n",
        "#  Add vehicle_type and operational_cost columns\n",
        "df_cost = df_congestion.withColumn(\n",
        "    \"vehicle_type\",\n",
        "    when(col(\"route_id\").rlike(\"R00[1-5]\"), \"Bus\")\n",
        "    .when(col(\"route_id\").rlike(\"R0[0-1][6-9]\"), \"Metro\")\n",
        "    .otherwise(\"Tram\")\n",
        ").withColumn(\n",
        "    \"cost\",\n",
        "    when(col(\"vehicle_type\") == \"Bus\", col(\"Passenger Count\") * 2.5 + 50)\n",
        "    .when(col(\"vehicle_type\") == \"Metro\", col(\"Passenger Count\") * 1.8 + 120)\n",
        "    .otherwise(col(\"Passenger Count\") * 2.0 + 80)\n",
        ")\n",
        "\n",
        "# Show the result\n",
        "df_cost.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udkHmlXFtweg",
        "outputId": "e32af82c-fb80-4495-e59b-210998e47236"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------------+-----------+------------+-----+\n",
            "|route_id|Passenger Count|OtherColumn|vehicle_type| cost|\n",
            "+--------+---------------+-----------+------------+-----+\n",
            "|    R001|             10|        100|         Bus| 75.0|\n",
            "|    R007|             15|        150|       Metro|147.0|\n",
            "|    R012|             20|        200|        Tram|120.0|\n",
            "+--------+---------------+-----------+------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Performance metrics to measure"
      ],
      "metadata": {
        "id": "mctO2m0P2OBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import (\n",
        "    col, when, avg, sum as spark_sum, count, max as spark_max, min as spark_min,\n",
        "    hour, dayofweek, stddev, percentile_approx, desc, asc,\n",
        "    lag, lead, window, collect_list, expr, round as spark_round, abs as spark_abs\n",
        ")"
      ],
      "metadata": {
        "id": "dYDE3m0D36lQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "import time\n",
        "import psutil\n",
        "import os\n",
        "import gc\n",
        "import threading\n",
        "import json\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "xF0By3de4IGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark Session with enhanced monitoring\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SmartCitySparkPerformanceAnalysis\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"10000\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"WARN\")"
      ],
      "metadata": {
        "id": "OcB5kr1e4K0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ComprehensivePerformanceMonitor:\n",
        "    \"\"\"\n",
        "    Enhanced performance monitoring system that tracks:\n",
        "    - Execution time with granular breakdowns\n",
        "    - Memory usage (RSS, VMS, peak usage)\n",
        "    - CPU utilization during operations\n",
        "    - Accuracy metrics for predictions\n",
        "    - Spark-specific metrics (tasks, stages, etc.)\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "P4I5aU0d4QAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def __init__(self, operation_name: str, enable_cpu_monitoring: bool = True):\n",
        "        self.operation_name = operation_name\n",
        "        self.enable_cpu_monitoring = enable_cpu_monitoring\n",
        "        self.start_time = None\n",
        "        self.end_time = None\n",
        "\n",
        "        # Memory metrics\n",
        "        self.start_memory_rss = None\n",
        "        self.start_memory_vms = None\n",
        "        self.peak_memory_rss = 0\n",
        "        self.peak_memory_vms = 0\n",
        "        self.memory_samples = []\n",
        "\n",
        "        # CPU metrics\n",
        "        self.cpu_samples = []\n",
        "        self.cpu_monitoring_thread = None\n",
        "        self.stop_cpu_monitoring = False\n",
        "\n",
        "        # Spark metrics\n",
        "        self.spark_metrics = {}\n",
        "\n",
        "        # Process reference\n",
        "        self.process = psutil.Process(os.getpid())\n",
        "\n",
        "        # Results storage\n",
        "        self.results = {}"
      ],
      "metadata": {
        "id": "UKLA5oE44S8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _monitor_resources(self):\n",
        "        \"\"\"Background thread to monitor CPU and memory continuously\"\"\"\n",
        "        while not self.stop_cpu_monitoring:\n",
        "\n",
        "                # CPU utilization\n",
        "                cpu_percent = self.process.cpu_percent()\n",
        "                self.cpu_samples.append(cpu_percent)\n",
        "\n",
        "                # Memory usage\n",
        "                memory_info = self.process.memory_info()\n",
        "                current_rss = memory_info.rss / 1024 / 1024  # MB\n",
        "                current_vms = memory_info.vms / 1024 / 1024  # MB\n",
        "\n",
        "                self.memory_samples.append({\n",
        "                    'timestamp': time.time(),\n",
        "                    'rss': current_rss,\n",
        "                    'vms': current_vms\n",
        "                })"
      ],
      "metadata": {
        "id": "G45oKhGU4XcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Initialize peak memory usage variables\n",
        "peak_memory_rss = 0\n",
        "peak_memory_vms = 0\n",
        "\n",
        "# Duration for monitoring (in seconds)\n",
        "monitor_duration = 10  # e.g., 10 seconds\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    process = psutil.Process(os.getpid())\n",
        "    while time.time() - start_time < monitor_duration:\n",
        "        current_rss = process.memory_info().rss\n",
        "        current_vms = process.memory_info().vms\n",
        "\n",
        "        # Track peak memory\n",
        "        peak_memory_rss = max(peak_memory_rss, current_rss)\n",
        "        peak_memory_vms = max(peak_memory_vms, current_vms)\n",
        "\n",
        "        time.sleep(0.1)  # Sample every 100ms\n",
        "except Exception as e:\n",
        "    print(f\"Resource monitoring error: {e}\")\n",
        "\n",
        "# After loop ends, print peak memory usage\n",
        "print(f\"Peak RSS memory: {peak_memory_rss / (1024 ** 2):.2f} MB\")\n",
        "print(f\"Peak VMS memory: {peak_memory_vms / (1024 ** 2):.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-oENeDy6FGp",
        "outputId": "349b03c1-14d9-4b3d-85d9-74d6ca0dffa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Peak RSS memory: 250.02 MB\n",
            "Peak VMS memory: 1637.74 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def __enter__(self):\n",
        "        # Reset collections\n",
        "        self.cpu_samples = []\n",
        "        self.memory_samples = []\n",
        "        self.peak_memory_rss = 0\n",
        "        self.peak_memory_vms = 0"
      ],
      "metadata": {
        "id": "owM_xWqX6K_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "class ResourceMonitor:\n",
        "    def __init__(self):\n",
        "        self.process = psutil.Process(os.getpid())\n",
        "        self.start_time = None\n",
        "        self.start_memory_rss = None\n",
        "        self.start_memory_vms = None\n",
        "\n",
        "    def start_measurements(self):\n",
        "        self.start_time = time.time()\n",
        "        initial_memory = self.process.memory_info()\n",
        "        self.start_memory_rss = initial_memory.rss / 1024 / 1024\n",
        "        self.start_memory_vms = initial_memory.vms / 1024 / 1024\n",
        "\n"
      ],
      "metadata": {
        "id": "noMNgAed6um7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "\n",
        "class YourClassName:\n",
        "    def __init__(self):\n",
        "        self.enable_cpu_monitoring = True  # or False, depending on your setting\n",
        "        self.stop_cpu_monitoring = False\n",
        "        self.cpu_monitoring_thread = None\n",
        "\n",
        "    def _monitor_resources(self):\n",
        "        # Implement resource monitoring logic here\n",
        "        while not self.stop_cpu_monitoring:\n",
        "            # Collect resource usage here\n",
        "            # e.g., CPU, memory, etc.\n",
        "            pass\n",
        "\n",
        "    def start_resource_monitoring(self):\n",
        "        # Start resource monitoring thread\n",
        "        if self.enable_cpu_monitoring:\n",
        "            self.stop_cpu_monitoring = False\n",
        "            self.cpu_monitoring_thread = threading.Thread(target=self._monitor_resources)\n",
        "            self.cpu_monitoring_thread.daemon = True\n",
        "            self.cpu_monitoring_thread.start()\n",
        "\n",
        "    def stop_resource_monitoring(self):\n",
        "        # To stop the thread gracefully\n",
        "        self.stop_cpu_monitoring = True\n",
        "        if self.cpu_monitoring_thread:\n",
        "            self.cpu_monitoring_thread.join()"
      ],
      "metadata": {
        "id": "kY426bse66tH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class YourClass:\n",
        "    def __init__(self, operation_name):\n",
        "        self.operation_name = operation_name\n",
        "\n",
        "    def _capture_spark_metrics(self, stage):\n",
        "        # Your code to capture Spark metrics\n",
        "        pass\n",
        "\n",
        "    def start_performance_monitoring(self):\n",
        "        # Get initial Spark metrics\n",
        "        self._capture_spark_metrics('start')\n",
        "\n",
        "        print(f\"\\nStarting performance monitoring for: {self.operation_name}\")\n",
        "        return self\n"
      ],
      "metadata": {
        "id": "2Ns2JEeu7H_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "\n",
        "    pass"
      ],
      "metadata": {
        "id": "CUxpFyrh7ezK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stop_monitoring(self):\n",
        "    # Record end time\n",
        "    self.end_time = time.time()\n",
        "\n",
        "    # Signal the thread to stop\n",
        "    self.stop_cpu_monitoring = True\n",
        "\n",
        "    # Wait for the thread to finish\n",
        "    if self.cpu_monitoring_thread and self.cpu_monitoring_thread.is_alive():\n",
        "        self.cpu_monitoring_thread.join(timeout=1.0)"
      ],
      "metadata": {
        "id": "4fqImuwf7ptz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def record_final_memory(self):\n",
        "    # Get final memory info\n",
        "    final_memory = self.process.memory_info()\n",
        "    final_memory_rss = final_memory.rss / 1024 / 1024\n",
        "    final_memory_vms = final_memory.vms / 1024 / 1024\n",
        "\n",
        "    return final_memory_rss, final_memory_vms"
      ],
      "metadata": {
        "id": "KJqWfZJJ70cE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil"
      ],
      "metadata": {
        "id": "z1QkO2lvGP52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def record_final_memory(self):\n",
        "    # Get final memory info\n",
        "    final_memory = self.process.memory_info()\n",
        "    final_memory_rss = final_memory.rss / 1024 / 1024\n",
        "    final_memory_vms = final_memory.vms / 1024 / 1024\n",
        "\n",
        "    print(f\"Final Memory - RSS: {final_memory_rss:.2f} MB, VMS: {final_memory_vms:.2f} MB\")\n",
        "\n",
        "    return final_memory_rss, final_memory_vms\n"
      ],
      "metadata": {
        "id": "0ObQ0LSlGuq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "\n",
        "class MemoryTracker:\n",
        "    def __init__(self):\n",
        "        self.process = psutil.Process()\n",
        "\n",
        "    def record_final_memory(self):\n",
        "        # Get final memory info\n",
        "        final_memory = self.process.memory_info()\n",
        "        final_memory_rss = final_memory.rss / 1024 / 1024\n",
        "        final_memory_vms = final_memory.vms / 1024 / 1024\n",
        "\n",
        "        print(f\"Final Memory - RSS: {final_memory_rss:.2f} MB, VMS: {final_memory_vms:.2f} MB\")\n",
        "        return final_memory_rss, final_memory_vms\n",
        "\n",
        "# Create an instance of the class\n",
        "tracker = MemoryTracker()\n",
        "\n",
        "# Call the method to print and return final memory\n",
        "rss, vms = tracker.record_final_memory()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZwEsq2dHPXD",
        "outputId": "3bf805a8-b88a-4114-c679-4904c9049224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Memory - RSS: 250.84 MB, VMS: 1638.74 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BX0gU4hTRHPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(self):\n",
        "    # Calculate metrics after execution\n",
        "    execution_time = self.end_time - self.start_time\n",
        "    memory_delta_rss = self.final_memory_rss - self.start_memory_rss\n",
        "    memory_delta_vms = self.final_memory_vms - self.start_memory_vms\n",
        "\n",
        "    print(f\"Execution Time: {execution_time:.2f} seconds\")\n",
        "    print(f\"Memory Change - RSS: {memory_delta_rss:.2f} MB\")\n",
        "    print(f\"Memory Change - VMS: {memory_delta_vms:.2f} MB\")\n",
        "\n",
        "    return execution_time, memory_delta_rss, memory_delta_vms\n"
      ],
      "metadata": {
        "id": "Lrmn-EEfHVzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CPU :\n",
        "    def __init__(self):\n",
        "        self.cpu_samples = []\n",
        "\n",
        "    def some_method(self):\n",
        "        cpu_stats = self._calculate_cpu_stats() if self.cpu_samples else {}\n",
        "        # rest of method"
      ],
      "metadata": {
        "id": "RVZSzI0CJNu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Memory :\n",
        "    def __init__(self):\n",
        "        self.memory_samples = []\n",
        "\n",
        "    def get_stats(self):\n",
        "\n",
        "        memory_stats = self._calculate_memory_stats()\n",
        "        return memory_stats"
      ],
      "metadata": {
        "id": "ousUXlkbJu3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Spark:\n",
        "    def __init__(self):\n",
        "        self.spark_samples = []\n",
        "\n",
        "    def compute_stats(self):\n",
        "        spark_stats = self._calculate_spark_stats()\n",
        "        return spark_stats"
      ],
      "metadata": {
        "id": "Hba24tYFKAq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "class PerformanceTracker:\n",
        "    def __init__(self, operation_name):\n",
        "        self.operation_name = operation_name\n",
        "        self.results = None\n",
        "\n",
        "    def record_metrics(\n",
        "        self,\n",
        "        execution_time,\n",
        "        final_memory_rss,\n",
        "        memory_delta_rss,\n",
        "        final_memory_vms,\n",
        "        memory_delta_vms,\n",
        "        memory_stats,\n",
        "        cpu_stats,\n",
        "        spark_stats\n",
        "    ):\n",
        "        # Properly indented (4 spaces)\n",
        "        self.results = {\n",
        "            'operation_name': self.operation_name,\n",
        "            'execution_time': execution_time,\n",
        "            'memory': {\n",
        "                'start_rss_mb': self.start_memory_rss,\n",
        "                'final_rss_mb': final_memory_rss,\n",
        "                'delta_rss_mb': memory_delta_rss,\n",
        "                'peak_rss_mb': self.peak_memory_rss,\n",
        "                'start_vms_mb': self.start_memory_vms,\n",
        "                'final_vms_mb': final_memory_vms,\n",
        "                'delta_vms_mb': memory_delta_vms,\n",
        "                'peak_vms_mb': self.peak_memory_vms,\n",
        "                **memory_stats\n",
        "            },\n",
        "            'cpu': cpu_stats,\n",
        "            'spark_metrics': spark_stats,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }"
      ],
      "metadata": {
        "id": "4f0yMLZuKXhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PerformanceMonitor:\n",
        "    def __init__(self):\n",
        "        self.results = None\n",
        "\n",
        "    def finalize(self, execution_time, memory_stats, cpu_stats, spark_stats):\n",
        "        # Store results\n",
        "        self.results = {\n",
        "            'execution_time': execution_time,\n",
        "            'memory': memory_stats,\n",
        "            'cpu': cpu_stats,\n",
        "            'spark': spark_stats\n",
        "        }\n",
        "\n",
        "        # Print summary\n",
        "        self._print_performance_summary()\n",
        "\n",
        "    def _print_performance_summary(self):\n",
        "        print(\"Performance Summary:\")\n",
        "        print(f\"Execution Time: {self.results['execution_time']:.2f}s\")\n"
      ],
      "metadata": {
        "id": "jHuOGGm_KsXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "class PerformanceMonitor:\n",
        "    def __init__(self, operation_name):\n",
        "        self.operation_name = operation_name\n",
        "        # Mock data initialization\n",
        "        self.start_memory_rss = 100.5\n",
        "        self.peak_memory_rss = 250.3\n",
        "        self.start_memory_vms = 200.0\n",
        "        self.peak_memory_vms = 500.0\n",
        "        self.results = None\n",
        "\n",
        "    def record_metrics(self, execution_time):\n",
        "        \"\"\"Mock method to record metrics with sample data\"\"\"\n",
        "        memory_stats = {'cache': 45.2, 'buffers': 30.1}\n",
        "        cpu_stats = {'user_usage': 75.5, 'system_usage': 12.3}\n",
        "        spark_stats = {'executor_memory': 8192, 'tasks_completed': 200}\n",
        "\n",
        "        self.results = {\n",
        "            'operation_name': self.operation_name,\n",
        "            'execution_time': execution_time,\n",
        "            'memory': {\n",
        "                'start_rss_mb': self.start_memory_rss,\n",
        "                'final_rss_mb': 220.7,\n",
        "                'delta_rss_mb': 120.2,\n",
        "                'peak_rss_mb': self.peak_memory_rss,\n",
        "                'start_vms_mb': self.start_memory_vms,\n",
        "                'final_vms_mb': 450.0,\n",
        "                'delta_vms_mb': 250.0,\n",
        "                'peak_vms_mb': self.peak_memory_vms,\n",
        "                **memory_stats\n",
        "            },\n",
        "            'cpu': cpu_stats,\n",
        "            'spark_metrics': spark_stats,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "        self._print_performance_summary()\n",
        "\n",
        "    def _print_performance_summary(self):\n",
        "        \"\"\"Prints a formatted performance summary\"\"\"\n",
        "        if not self.results:\n",
        "            print(\"No performance data available!\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"PERFORMANCE SUMMARY\".center(60))\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Basic Info\n",
        "        print(f\"\\n{'Operation:':<15}{self.results['operation_name']}\")\n",
        "        print(f\"{'Timestamp:':<15}{self.results['timestamp']}\")\n",
        "        print(f\"{'Duration:':<15}{self.results['execution_time']:.2f} seconds\")\n",
        "\n",
        "        # Memory\n",
        "        print(\"\\nMEMORY USAGE (MB):\")\n",
        "        mem = self.results['memory']\n",
        "        print(f\"{'':<10}{'Start':>10}{'Final':>10}{'Delta':>10}{'Peak':>10}\")\n",
        "        print(f\"{'RSS:':<10}{mem['start_rss_mb']:>10.1f}{mem['final_rss_mb']:>10.1f}{mem['delta_rss_mb']:>10.1f}{mem['peak_rss_mb']:>10.1f}\")\n",
        "        print(f\"{'VMS:':<10}{mem['start_vms_mb']:>10.1f}{mem['final_vms_mb']:>10.1f}{mem['delta_vms_mb']:>10.1f}{mem['peak_vms_mb']:>10.1f}\")\n",
        "\n",
        "        # CPU\n",
        "        if self.results.get('cpu'):\n",
        "            print(\"\\nCPU USAGE (%):\")\n",
        "            for metric, value in self.results['cpu'].items():\n",
        "                print(f\"{metric.replace('_', ' ').title():<20}{value:>5.1f}%\")\n",
        "\n",
        "        # Spark\n",
        "        if self.results.get('spark_metrics'):\n",
        "            print(\"\\nSPARK METRICS:\")\n",
        "            for metric, value in self.results['spark_metrics'].items():\n",
        "                print(f\"{metric.replace('_', ' ').title():<25}{value:>10}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Test it\n",
        "if __name__ == \"__main__\":\n",
        "    monitor = PerformanceMonitor(\"data_processing_job\")\n",
        "    monitor.record_metrics(12.34)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5iNGr1tMGuM",
        "outputId": "a152ae5c-cd74-4dcc-cc2d-41293b7cf5e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "                    PERFORMANCE SUMMARY                     \n",
            "============================================================\n",
            "\n",
            "Operation:     data_processing_job\n",
            "Timestamp:     2025-07-25T06:11:24.437896\n",
            "Duration:      12.34 seconds\n",
            "\n",
            "MEMORY USAGE (MB):\n",
            "               Start     Final     Delta      Peak\n",
            "RSS:           100.5     220.7     120.2     250.3\n",
            "VMS:           200.0     450.0     250.0     500.0\n",
            "\n",
            "CPU USAGE (%):\n",
            "User Usage           75.5%\n",
            "System Usage         12.3%\n",
            "\n",
            "SPARK METRICS:\n",
            "Executor Memory                8192\n",
            "Tasks Completed                 200\n",
            "\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Hive** Queries for Transport **Analytics**"
      ],
      "metadata": {
        "id": "b-tkvaCUZSKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeTnsgHqqOMH",
        "outputId": "a4e360af-1065-4d48-bf46-e2742e753a59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 39.7 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u452-ga~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u452-ga~us1-0ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u452-ga~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u452-ga~us1-0ubuntu1~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u452-ga~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u452-ga~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Create Spark session with Hive support\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"TransportationAnalysis\") \\\n",
        "    .config(\"spark.sql.warehouse.dir\", \"/content/warehouse\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "ZMRgrjx1qjkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Database Schema"
      ],
      "metadata": {
        "id": "hKBgMUGNuTXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Initialize Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"TransportationAnalysis\") \\\n",
        "    .config(\"spark.sql.warehouse.dir\", \"/content/warehouse\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create tables with corrected syntax\n",
        "spark.sql(\"DROP TABLE IF EXISTS fact_trips\")\n",
        "spark.sql(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS fact_trips (\n",
        "    trip_id STRING,\n",
        "    vehicle_id STRING,\n",
        "    driver_id STRING,\n",
        "    route_id STRING,\n",
        "    start_zone_id STRING,\n",
        "    end_zone_id STRING,\n",
        "    start_time TIMESTAMP,\n",
        "    end_time TIMESTAMP,\n",
        "    distance_km DOUBLE,\n",
        "    fare_amount DOUBLE,\n",
        "    toll_amount DOUBLE,\n",
        "    surcharge_amount DOUBLE,\n",
        "    total_amount DOUBLE,\n",
        "    passenger_count INT,\n",
        "    weather_condition STRING,\n",
        "    event_id STRING,\n",
        "    delay_minutes INT\n",
        ") USING PARQUET\n",
        "\"\"\")\n",
        "\n",
        "spark.sql(\"DROP TABLE IF EXISTS dim_zones\")\n",
        "spark.sql(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS dim_zones (\n",
        "    zone_id STRING,\n",
        "    zone_name STRING,\n",
        "    zone_type STRING,\n",
        "    population_density STRING,\n",
        "    income_level STRING,\n",
        "    business_percentage DOUBLE\n",
        ") USING PARQUET\n",
        "\"\"\")\n",
        "\n",
        "# Sample data with proper formatting\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Create sample data as Row objects\n",
        "trips_data = [\n",
        "    Row(trip_id=\"T1\", vehicle_id=\"V1\", driver_id=\"D1\", route_id=\"R1\",\n",
        "        start_zone_id=\"Z1\", end_zone_id=\"Z2\",\n",
        "        start_time=\"2023-01-01 08:15:00\", end_time=\"2023-01-01 08:45:00\",\n",
        "        distance_km=10.5, fare_amount=15.0, toll_amount=2.0, surcharge_amount=1.5,\n",
        "        total_amount=18.5, passenger_count=2, weather_condition=\"Rain\",\n",
        "        event_id=None, delay_minutes=5),\n",
        "    Row(trip_id=\"T2\", vehicle_id=\"V2\", driver_id=\"D2\", route_id=\"R2\",\n",
        "        start_zone_id=\"Z2\", end_zone_id=\"Z3\",\n",
        "        start_time=\"2023-01-01 09:30:00\", end_time=\"2023-01-01 10:15:00\",\n",
        "        distance_km=15.0, fare_amount=22.0, toll_amount=3.0, surcharge_amount=2.0,\n",
        "        total_amount=27.0, passenger_count=3, weather_condition=\"Clear\",\n",
        "        event_id=\"E1\", delay_minutes=12)\n",
        "]\n",
        "\n",
        "zones_data = [\n",
        "    Row(zone_id=\"Z1\", zone_name=\"Downtown\", zone_type=\"Commercial\",\n",
        "        population_density=\"High\", income_level=\"High\", business_percentage=70.0),\n",
        "    Row(zone_id=\"Z2\", zone_name=\"Midtown\", zone_type=\"Mixed\",\n",
        "        population_density=\"Medium\", income_level=\"Medium\", business_percentage=50.0),\n",
        "    Row(zone_id=\"Z3\", zone_name=\"Suburb\", zone_type=\"Residential\",\n",
        "        population_density=\"Low\", income_level=\"High\", business_percentage=20.0)\n",
        "]\n",
        "\n",
        "# Create DataFrames\n",
        "trips_df = spark.createDataFrame(trips_data)\n",
        "zones_df = spark.createDataFrame(zones_data)\n",
        "\n",
        "# Write to tables\n",
        "trips_df.write.mode(\"overwrite\").saveAsTable(\"fact_trips\")\n",
        "zones_df.write.mode(\"overwrite\").saveAsTable(\"dim_zones\")\n",
        "\n",
        "# Register UDF\n",
        "def peak_hour_classifier(time_str):\n",
        "    from datetime import datetime\n",
        "    if not time_str:\n",
        "        return \"Off-Peak\"\n",
        "    try:\n",
        "        dt = datetime.strptime(time_str, \"%Y-%m-%d %H:%M:%S\")\n",
        "        hour = dt.hour\n",
        "        if 7 <= hour <= 9: return \"Morning Peak\"\n",
        "        if 16 <= hour <= 19: return \"Evening Peak\"\n",
        "        if 11 <= hour <= 13: return \"Lunch Peak\"\n",
        "        return \"Off-Peak\"\n",
        "    except:\n",
        "        return \"Off-Peak\"\n",
        "\n",
        "spark.udf.register(\"peak_hour\", peak_hour_classifier)\n",
        "\n",
        "# Run a sample query\n",
        "result = spark.sql(\"\"\"\n",
        "SELECT\n",
        "    start_zone_id,\n",
        "    peak_hour(start_time) AS time_period,\n",
        "    COUNT(*) AS trip_count,\n",
        "    AVG(delay_minutes) AS avg_delay\n",
        "FROM\n",
        "    fact_trips\n",
        "GROUP BY\n",
        "    start_zone_id, peak_hour(start_time)\n",
        "ORDER BY\n",
        "    start_zone_id, time_period\n",
        "\"\"\")\n",
        "\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0quuT_IFrouA",
        "outputId": "038665db-0936-49f9-da69-f97c0be84d6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "openjdk-8-jdk-headless is already the newest version (8u452-ga~us1-0ubuntu1~22.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "+-------------+------------+----------+---------+\n",
            "|start_zone_id| time_period|trip_count|avg_delay|\n",
            "+-------------+------------+----------+---------+\n",
            "|           Z1|Morning Peak|         1|      5.0|\n",
            "|           Z2|Morning Peak|         1|     12.0|\n",
            "+-------------+------------+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Peak Hour Classification"
      ],
      "metadata": {
        "id": "9x8B4giNw26h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf, col\n",
        "from pyspark.sql.types import StringType\n",
        "from datetime import datetime\n",
        "\n",
        "# Initialize Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PeakHourAnalysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [(\"T1\", \"2023-01-01 08:15:00\"),\n",
        "        (\"T2\", \"2023-01-01 17:30:00\"),\n",
        "        (\"T3\", \"2023-01-01 12:00:00\"),\n",
        "        (\"T4\", \"2023-01-01 14:00:00\")]\n",
        "\n",
        "# Create DataFrame\n",
        "columns = [\"trip_id\", \"start_time\"]\n",
        "trips_df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Define and register UDF\n",
        "def classify_peak_hour(time_str):\n",
        "    try:\n",
        "        dt = datetime.strptime(time_str, \"%Y-%m-%d %H:%M:%S\")\n",
        "        hour = dt.hour\n",
        "        if 7 <= hour <= 9: return \"Morning Peak\"\n",
        "        if 16 <= hour <= 19: return \"Evening Peak\"\n",
        "        if 11 <= hour <= 13: return \"Lunch Peak\"\n",
        "        return \"Off-Peak\"\n",
        "    except:\n",
        "        return \"Unknown\"\n",
        "\n",
        "peak_hour_udf = udf(classify_peak_hour, StringType())\n",
        "spark.udf.register(\"peak_hour\", classify_peak_hour, StringType())\n",
        "\n",
        "# Using the UDF\n",
        "trips_with_peak = trips_df.withColumn(\"time_period\", peak_hour_udf(col(\"start_time\")))\n",
        "trips_with_peak.show()\n",
        "\n",
        "# Using in SQL\n",
        "trips_df.createOrReplaceTempView(\"trips\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT trip_id, start_time, peak_hour(start_time) AS period\n",
        "    FROM trips\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dpv86HPnskEF",
        "outputId": "dd659301-4050-4738-fdc4-a7b6a3421c48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "+-------+-------------------+------------+\n",
            "|trip_id|         start_time| time_period|\n",
            "+-------+-------------------+------------+\n",
            "|     T1|2023-01-01 08:15:00|Morning Peak|\n",
            "|     T2|2023-01-01 17:30:00|Evening Peak|\n",
            "|     T3|2023-01-01 12:00:00|  Lunch Peak|\n",
            "|     T4|2023-01-01 14:00:00|    Off-Peak|\n",
            "+-------+-------------------+------------+\n",
            "\n",
            "+-------+-------------------+------------+\n",
            "|trip_id|         start_time|      period|\n",
            "+-------+-------------------+------------+\n",
            "|     T1|2023-01-01 08:15:00|Morning Peak|\n",
            "|     T2|2023-01-01 17:30:00|Evening Peak|\n",
            "|     T3|2023-01-01 12:00:00|  Lunch Peak|\n",
            "|     T4|2023-01-01 14:00:00|    Off-Peak|\n",
            "+-------+-------------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Delay Zone Analysis"
      ],
      "metadata": {
        "id": "Ol5IYEuAub6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg, count, sum as spark_sum, when, expr\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BottleneckAnalysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# sample data\n",
        "from pyspark.sql import Row\n",
        "from datetime import date\n",
        "\n",
        "# Sample trip data\n",
        "trips_data = [\n",
        "    Row(trip_id=\"T1\", start_zone_id=\"Z1\", trip_date=date(2023, 1, 1), delay_minutes=10),\n",
        "    Row(trip_id=\"T2\", start_zone_id=\"Z1\", trip_date=date(2023, 1, 2), delay_minutes=25),\n",
        "    Row(trip_id=\"T3\", start_zone_id=\"Z2\", trip_date=date(2023, 1, 1), delay_minutes=5),\n",
        "    Row(trip_id=\"T4\", start_zone_id=\"Z2\", trip_date=date(2023, 1, 3), delay_minutes=30),\n",
        "    Row(trip_id=\"T5\", start_zone_id=\"Z1\", trip_date=date(2023, 1, 4), delay_minutes=18),\n",
        "    # Add more sample data as needed\n",
        "]\n",
        "\n",
        "# Sample zone data\n",
        "zones_data = [\n",
        "    Row(zone_id=\"Z1\", zone_name=\"Downtown\", zone_type=\"Commercial\"),\n",
        "    Row(zone_id=\"Z2\", zone_name=\"Midtown\", zone_type=\"Residential\"),\n",
        "]\n",
        "\n",
        "# Create DataFrames\n",
        "fact_trips_df = spark.createDataFrame(trips_data)\n",
        "dim_zones_df = spark.createDataFrame(zones_data)\n",
        "\n",
        "# Register temporary views for SQL queries\n",
        "fact_trips_df.createOrReplaceTempView(\"fact_trips\")\n",
        "dim_zones_df.createOrReplaceTempView(\"dim_zones\")\n",
        "\n",
        "# Run the analysis using DataFrame API\n",
        "bottleneck_results = (\n",
        "    fact_trips_df.alias(\"t\")\n",
        "    .join(\n",
        "        dim_zones_df.alias(\"z\"),\n",
        "        col(\"t.start_zone_id\") == col(\"z.zone_id\"),\n",
        "        \"inner\"\n",
        "    )\n",
        "    .filter(\n",
        "        (col(\"t.trip_date\").between(\"2023-01-01\", \"2023-12-31\")) &\n",
        "        (col(\"t.delay_minutes\") > 0)\n",
        "    )\n",
        "    .groupBy(\"z.zone_name\", \"z.zone_type\")\n",
        "    .agg(\n",
        "        count(\"t.trip_id\").alias(\"total_trips\"),\n",
        "        avg(\"t.delay_minutes\").alias(\"avg_delay_minutes\"),\n",
        "        expr(\"percentile_approx(t.delay_minutes, 0.95)\").alias(\"p95_delay_minutes\"),\n",
        "        spark_sum(when(col(\"t.delay_minutes\") > 15, 1).otherwise(0)).alias(\"delayed_trips_count\"),\n",
        "        (spark_sum(when(col(\"t.delay_minutes\") > 15, 1).otherwise(0)) / count(\"t.trip_id\")).alias(\"delay_probability\")\n",
        "    )\n",
        "    .filter(col(\"total_trips\") > 0)  # Changed from >100 for sample data\n",
        "    .orderBy(\n",
        "        col(\"delay_probability\").desc(),\n",
        "        col(\"avg_delay_minutes\").desc()\n",
        "    )\n",
        "    .limit(10)\n",
        ")\n",
        "\n",
        "# Show results\n",
        "bottleneck_results.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rym-GeV9t9Xf",
        "outputId": "3fc52b3c-f70b-48ae-9ab3-bdf48545e98c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------+-----------+------------------+-----------------+-------------------+------------------+\n",
            "|zone_name|  zone_type|total_trips| avg_delay_minutes|p95_delay_minutes|delayed_trips_count| delay_probability|\n",
            "+---------+-----------+-----------+------------------+-----------------+-------------------+------------------+\n",
            "| Downtown| Commercial|          3|17.666666666666668|               25|                  2|0.6666666666666666|\n",
            "|  Midtown|Residential|          2|              17.5|               30|                  1|               0.5|\n",
            "+---------+-----------+-----------+------------------+-----------------+-------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time-of-Day Traffic Volume Trends with Peak Classification"
      ],
      "metadata": {
        "id": "t7NfNVvkwUpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import hour, expr\n",
        "\n",
        "# First add a timestamp column if possibl\n",
        "fact_trips_df = fact_trips_df.withColumn(\n",
        "    \"start_time\",\n",
        "    expr(\"to_timestamp(concat(cast(trip_date as string), ' 12:00:00'))\")  # Using noon as default time\n",
        ")\n",
        "\n",
        "# Then register the temporary view\n",
        "fact_trips_df.createOrReplaceTempView(\"fact_trips\")\n",
        "\n",
        "# Now run the time analysis\n",
        "time_analysis_query = \"\"\"\n",
        "SELECT\n",
        "    hour(start_time) AS hour_of_day,\n",
        "    COUNT(trip_id) AS trip_count,\n",
        "    AVG(delay_minutes) AS avg_delay\n",
        "FROM\n",
        "    fact_trips\n",
        "WHERE\n",
        "    trip_date BETWEEN '2023-01-01' AND '2023-12-31'\n",
        "GROUP BY\n",
        "    hour(start_time)\n",
        "ORDER BY\n",
        "    hour_of_day\n",
        "\"\"\"\n",
        "\n",
        "time_analysis_results = spark.sql(time_analysis_query)\n",
        "time_analysis_results.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0PWFB7XvV1Z",
        "outputId": "03619101-6a57-41aa-c11b-faea3bcbe362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+---------+\n",
            "|hour_of_day|trip_count|avg_delay|\n",
            "+-----------+----------+---------+\n",
            "|         12|         5|     17.6|\n",
            "+-----------+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Revenue Analysis by Route"
      ],
      "metadata": {
        "id": "PuOpHNzNvoXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count, sum, avg, lit, rand\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"RevenueAnalysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "from pyspark.sql import Row\n",
        "from datetime import date\n",
        "\n",
        "# Sample trip data with all required columns\n",
        "trips_data = [\n",
        "    Row(trip_id=\"T1\", route_id=\"R1\", start_zone_id=\"Z1\", end_zone_id=\"Z2\",\n",
        "        trip_date=date(2023, 1, 1), delay_minutes=10, total_amount=25.50, distance_km=12.3),\n",
        "    Row(trip_id=\"T2\", route_id=\"R1\", start_zone_id=\"Z1\", end_zone_id=\"Z3\",\n",
        "        trip_date=date(2023, 1, 2), delay_minutes=15, total_amount=30.00, distance_km=15.0),\n",
        "    Row(trip_id=\"T3\", route_id=\"R2\", start_zone_id=\"Z2\", end_zone_id=\"Z1\",\n",
        "        trip_date=date(2023, 1, 3), delay_minutes=5, total_amount=18.75, distance_km=8.5),\n",
        "    # Add more sample data as needed\n",
        "]\n",
        "\n",
        "# Sample route data\n",
        "routes_data = [\n",
        "    Row(route_id=\"R1\", route_name=\"Downtown Express\", origin_zone=\"Z1\", destination_zone=\"Z3\"),\n",
        "    Row(route_id=\"R2\", route_name=\"Cross-Town\", origin_zone=\"Z2\", destination_zone=\"Z1\"),\n",
        "]\n",
        "\n",
        "# Sample zone data\n",
        "zones_data = [\n",
        "    Row(zone_id=\"Z1\", zone_name=\"Central Business District\", zone_type=\"Commercial\"),\n",
        "    Row(zone_id=\"Z2\", zone_name=\"Midtown\", zone_type=\"Mixed\"),\n",
        "    Row(zone_id=\"Z3\", zone_name=\"Uptown\", zone_type=\"Residential\"),\n",
        "]\n",
        "\n",
        "# Create DataFrames\n",
        "fact_trips_df = spark.createDataFrame(trips_data)\n",
        "dim_routes_df = spark.createDataFrame(routes_data)\n",
        "dim_zones_df = spark.createDataFrame(zones_data)\n",
        "\n",
        "# Register temporary views\n",
        "fact_trips_df.createOrReplaceTempView(\"fact_trips\")\n",
        "dim_routes_df.createOrReplaceTempView(\"dim_routes\")\n",
        "dim_zones_df.createOrReplaceTempView(\"dim_zones\")\n",
        "\n",
        "# Perform the revenue analysis\n",
        "revenue_results = spark.sql(\"\"\"\n",
        "SELECT\n",
        "    r.route_name,\n",
        "    oz.zone_name AS origin_zone,\n",
        "    dz.zone_name AS destination_zone,\n",
        "    COUNT(t.trip_id) AS trip_count,\n",
        "    SUM(t.total_amount) AS total_revenue,\n",
        "    ROUND(SUM(t.total_amount) / COUNT(t.trip_id), 2) AS avg_revenue_per_trip,\n",
        "    ROUND(SUM(t.total_amount) / SUM(t.distance_km), 2) AS revenue_per_km,\n",
        "    SUM(t.distance_km) AS total_distance_km,\n",
        "    ROUND(AVG(t.delay_minutes), 2) AS avg_delay_minutes\n",
        "FROM\n",
        "    fact_trips t\n",
        "JOIN\n",
        "    dim_routes r ON t.route_id = r.route_id\n",
        "JOIN\n",
        "    dim_zones oz ON t.start_zone_id = oz.zone_id\n",
        "JOIN\n",
        "    dim_zones dz ON t.end_zone_id = dz.zone_id\n",
        "WHERE\n",
        "    t.trip_date BETWEEN '2023-01-01' AND '2023-12-31'\n",
        "GROUP BY\n",
        "    r.route_name, oz.zone_name, dz.zone_name\n",
        "HAVING\n",
        "    COUNT(t.trip_id) > 1  -- Changed from 50 for demo with small dataset\n",
        "ORDER BY\n",
        "    total_revenue DESC\n",
        "LIMIT 20\n",
        "\"\"\")\n",
        "\n",
        "# Show results\n",
        "revenue_results.show(truncate=False)\n",
        "\n",
        "# Alternative DataFrame API version\n",
        "df_revenue_results = (\n",
        "    fact_trips_df.alias(\"t\")\n",
        "    .join(dim_routes_df.alias(\"r\"), col(\"t.route_id\") == col(\"r.route_id\"))\n",
        "    .join(dim_zones_df.alias(\"oz\"), col(\"t.start_zone_id\") == col(\"oz.zone_id\"))\n",
        "    .join(dim_zones_df.alias(\"dz\"), col(\"t.end_zone_id\") == col(\"dz.zone_id\"))\n",
        "    .filter(col(\"t.trip_date\").between(\"2023-01-01\", \"2023-12-31\"))\n",
        "    .groupBy(\"r.route_name\", \"oz.zone_name\", \"dz.zone_name\")\n",
        "    .agg(\n",
        "        count(\"t.trip_id\").alias(\"trip_count\"),\n",
        "        sum(\"t.total_amount\").alias(\"total_revenue\"),\n",
        "        avg(\"t.total_amount\").alias(\"avg_revenue_per_trip\"),\n",
        "        (sum(\"t.total_amount\")/sum(\"t.distance_km\")).alias(\"revenue_per_km\"),\n",
        "        sum(\"t.distance_km\").alias(\"total_distance_km\"),\n",
        "        avg(\"t.delay_minutes\").alias(\"avg_delay_minutes\")\n",
        "    )\n",
        "    .filter(col(\"trip_count\") > 1)\n",
        "    .orderBy(col(\"total_revenue\").desc())\n",
        "    .limit(20)\n",
        ")\n",
        "\n",
        "df_revenue_results.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmXvf_s_yJCK",
        "outputId": "0fbe1d9b-5cea-4829-970a-3c1749cb286f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+----------------+----------+-------------+--------------------+--------------+-----------------+-----------------+\n",
            "|route_name|origin_zone|destination_zone|trip_count|total_revenue|avg_revenue_per_trip|revenue_per_km|total_distance_km|avg_delay_minutes|\n",
            "+----------+-----------+----------------+----------+-------------+--------------------+--------------+-----------------+-----------------+\n",
            "+----------+-----------+----------------+----------+-------------+--------------------+--------------+-----------------+-----------------+\n",
            "\n",
            "+----------+---------+---------+----------+-------------+--------------------+--------------+-----------------+-----------------+\n",
            "|route_name|zone_name|zone_name|trip_count|total_revenue|avg_revenue_per_trip|revenue_per_km|total_distance_km|avg_delay_minutes|\n",
            "+----------+---------+---------+----------+-------------+--------------------+--------------+-----------------+-----------------+\n",
            "+----------+---------+---------+----------+-------------+--------------------+--------------+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Impact Analysis"
      ],
      "metadata": {
        "id": "Opf9ISNbyqy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from datetime import date\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ImpactAnalysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create sample data for all required tables\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Sample trip data\n",
        "trips_data = [\n",
        "    Row(trip_id=\"T1\", start_zone_id=\"Z1\", trip_date=date(2023, 1, 1),\n",
        "        delay_minutes=10, total_amount=25.50, passenger_count=2, event_id=None),\n",
        "    Row(trip_id=\"T2\", start_zone_id=\"Z2\", trip_date=date(2023, 1, 2),\n",
        "        delay_minutes=15, total_amount=30.00, passenger_count=3, event_id=\"E1\"),\n",
        "    Row(trip_id=\"T3\", start_zone_id=\"Z1\", trip_date=date(2023, 1, 3),\n",
        "        delay_minutes=5, total_amount=18.75, passenger_count=1, event_id=None),\n",
        "    # Add more sample data as needed\n",
        "]\n",
        "\n",
        "# Sample weather data\n",
        "weather_data = [\n",
        "    Row(weather_date=date(2023, 1, 1), weather_condition=\"Sunny\"),\n",
        "    Row(weather_date=date(2023, 1, 2), weather_condition=\"Rainy\"),\n",
        "    Row(weather_date=date(2023, 1, 3), weather_condition=\"Cloudy\"),\n",
        "]\n",
        "\n",
        "# Sample zone data (extended with population_density and income_level)\n",
        "zones_data = [\n",
        "    Row(zone_id=\"Z1\", zone_name=\"Downtown\", zone_type=\"Commercial\",\n",
        "        population_density=\"High\", income_level=\"High\"),\n",
        "    Row(zone_id=\"Z2\", zone_name=\"Midtown\", zone_type=\"Residential\",\n",
        "        population_density=\"Medium\", income_level=\"Middle\"),\n",
        "]\n",
        "\n",
        "# Sample events data\n",
        "events_data = [\n",
        "    Row(event_id=\"E1\", event_name=\"Music Festival\", event_type=\"Concert\"),\n",
        "]\n",
        "\n",
        "# Create DataFrames\n",
        "fact_trips_df = spark.createDataFrame(trips_data)\n",
        "dim_weather_df = spark.createDataFrame(weather_data)\n",
        "dim_zones_df = spark.createDataFrame(zones_data)\n",
        "dim_events_df = spark.createDataFrame(events_data)\n",
        "\n",
        "# Register temporary views\n",
        "fact_trips_df.createOrReplaceTempView(\"fact_trips\")\n",
        "dim_weather_df.createOrReplaceTempView(\"dim_weather\")\n",
        "dim_zones_df.createOrReplaceTempView(\"dim_zones\")\n",
        "dim_events_df.createOrReplaceTempView(\"dim_events\")\n",
        "\n",
        "# Run the impact analysis query\n",
        "impact_analysis = spark.sql(\"\"\"\n",
        "SELECT\n",
        "    w.weather_condition,\n",
        "    z.population_density,\n",
        "    z.income_level,\n",
        "    COUNT(t.trip_id) AS trip_count,\n",
        "    ROUND(AVG(t.total_amount), 2) AS avg_fare,\n",
        "    ROUND(AVG(t.delay_minutes), 2) AS avg_delay,\n",
        "    ROUND(AVG(t.passenger_count), 2) AS avg_passengers\n",
        "FROM\n",
        "    fact_trips t\n",
        "JOIN\n",
        "    dim_weather w ON t.trip_date = w.weather_date\n",
        "JOIN\n",
        "    dim_zones z ON t.start_zone_id = z.zone_id\n",
        "LEFT JOIN\n",
        "    dim_events e ON t.event_id = e.event_id\n",
        "WHERE\n",
        "    t.trip_date BETWEEN '2023-01-01' AND '2023-12-31'\n",
        "    AND e.event_id IS NULL -- Exclude days with major events\n",
        "GROUP BY\n",
        "    w.weather_condition, z.population_density, z.income_level\n",
        "ORDER BY\n",
        "    trip_count DESC\n",
        "\"\"\")\n",
        "\n",
        "# Show results\n",
        "impact_analysis.show()\n",
        "\n",
        "# Alternative DataFrame API version\n",
        "df_impact_analysis = (\n",
        "    fact_trips_df.alias(\"t\")\n",
        "    .join(dim_weather_df.alias(\"w\"), col(\"t.trip_date\") == col(\"w.weather_date\"))\n",
        "    .join(dim_zones_df.alias(\"z\"), col(\"t.start_zone_id\") == col(\"z.zone_id\"))\n",
        "    .join(dim_events_df.alias(\"e\"), col(\"t.event_id\") == col(\"e.event_id\"), \"left\")\n",
        "    .filter(\n",
        "        (col(\"t.trip_date\").between(\"2023-01-01\", \"2023-12-31\")) &\n",
        "        (col(\"e.event_id\").isNull())\n",
        "    )\n",
        "    .groupBy(\"w.weather_condition\", \"z.population_density\", \"z.income_level\")\n",
        "    .agg(\n",
        "        count(\"t.trip_id\").alias(\"trip_count\"),\n",
        "        avg(\"t.total_amount\").alias(\"avg_fare\"),\n",
        "        avg(\"t.delay_minutes\").alias(\"avg_delay\"),\n",
        "        avg(\"t.passenger_count\").alias(\"avg_passengers\")\n",
        "    )\n",
        "    .orderBy(col(\"trip_count\").desc())\n",
        ")\n",
        "\n",
        "df_impact_analysis.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jgy7Us-czUZy",
        "outputId": "86191aa2-e49a-4c32-bcec-ea1c01832449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+------------------+------------+----------+--------+---------+--------------+\n",
            "|weather_condition|population_density|income_level|trip_count|avg_fare|avg_delay|avg_passengers|\n",
            "+-----------------+------------------+------------+----------+--------+---------+--------------+\n",
            "|            Sunny|              High|        High|         1|    25.5|     10.0|           2.0|\n",
            "|           Cloudy|              High|        High|         1|   18.75|      5.0|           1.0|\n",
            "+-----------------+------------------+------------+----------+--------+---------+--------------+\n",
            "\n",
            "+-----------------+------------------+------------+----------+--------+---------+--------------+\n",
            "|weather_condition|population_density|income_level|trip_count|avg_fare|avg_delay|avg_passengers|\n",
            "+-----------------+------------------+------------+----------+--------+---------+--------------+\n",
            "|            Sunny|              High|        High|         1|    25.5|     10.0|           2.0|\n",
            "|           Cloudy|              High|        High|         1|   18.75|      5.0|           1.0|\n",
            "+-----------------+------------------+------------+----------+--------+---------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iwr4orN4zcTy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}